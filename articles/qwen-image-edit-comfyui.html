<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>I Ran an AI Image Editor on My Mac Mini (And It Actually Worked) - Bahushruth CS</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="icon" type="image/svg+xml" href="../potato.svg">
</head>
<body>
    <nav class="nav">
        <div class="nav-container">
            <a href="../index.html" class="logo" title="Home">
                <img src="../potato.svg" alt="Potato" class="potato-icon">
            </a>
            <div class="nav-links">
                <a href="../articles.html" class="nav-link">Articles</a>
                <a href="../reading.html" class="nav-link">Reading</a>
                <a href="../about.html" class="nav-link">About</a>
            </div>
        </div>
    </nav>

    <main class="main">
        <article class="article-full">
            <a href="../articles.html" class="back-link">‚Üê Back to articles</a>
            
            <header class="article-header">
                <h1 class="article-title-large">I Ran an AI Image Editor on My Mac Mini (And It Actually Worked)</h1>
                <p class="article-meta-large">February 10, 2026 ¬∑ 12 min read</p>
            </header>

            <div class="article-content">
                <p class="lead">A few days ago, I decided to see if I could run Alibaba's new Qwen Image Edit model locally on my Mac Mini. No cloud APIs, no subscriptions, just pure local inference. Here's what happened.</p>

                <h2>The Setup</h2>
                <p>I've got a Mac Mini M2 Pro with 16GB unified memory. Not exactly a beefy machine for AI workloads, but I wanted to see how far I could push it. The goal was simple: take a photo, give the AI a text instruction like "make this person smile," and get an edited image back.</p>

                <p>Turns out, it's totally doable. But getting there required understanding a bunch of pieces that fit together in a very specific way. Let me walk you through it.</p>

                <h2>What You Need to Download</h2>
                <p>First things first, you need the actual model files. These aren't small, so make sure you have about 20GB free:</p>

                <table>
                    <tr>
                        <th>File</th>
                        <th>Size</th>
                        <th>Download</th>
                    </tr>
                    <tr>
                        <td>Qwen-Image-Edit-2509-Q8_0.gguf</td>
                        <td>~7.8GB</td>
                        <td><a href="https://huggingface.co/city96/Qwen-Image-Edit-gguf/tree/main" target="_blank">Hugging Face</a></td>
                    </tr>
                    <tr>
                        <td>qwen2.5-vl-7b-it-q8_0.gguf</td>
                        <td>~7.7GB</td>
                        <td><a href="https://huggingface.co/city96/Qwen2.5-VL-gguf/tree/main" target="_blank">Hugging Face</a></td>
                    </tr>
                    <tr>
                        <td>qwen_image_vae.safetensors</td>
                        <td>~500MB</td>
                        <td><a href="https://huggingface.co/city96/Qwen-Image-Edit-gguf/tree/main" target="_blank">Hugging Face</a></td>
                    </tr>
                    <tr>
                        <td>Qwen-Lightning-4steps LoRA</td>
                        <td>~4MB</td>
                        <td><a href="https://huggingface.co/city96/Qwen-Image-Edit-gguf/tree/main" target="_blank">Hugging Face</a></td>
                    </tr>
                </table>

                <h2>The Magic of GGUF</h2>
                <p>Notice those <code>.gguf</code> file extensions? That's the secret sauce. GGUF (GPT-Generated Unified Format) is a quantization format that shrinks massive AI models down to a size that can actually run on consumer hardware.</p>

                <p>The "Q8_0" in the filename means 8-bit quantization. Without this, you'd need a machine with 32GB+ of VRAM. With it? My 16GB Mac Mini handles it just fine. The quality loss is barely perceptible for most use cases.</p>

                <h2>Why ComfyUI?</h2>
                <p>I could have written Python scripts to run this model, but ComfyUI makes the whole process visual and way less painful. Think of it like node-based programming: you connect boxes together, each box does one thing, and the flow of data between them creates your image.</p>

                <p>Plus, ComfyUI has a thriving ecosystem of custom nodes. For this project, you'll need:</p>
                <ul>
                    <li><strong>ComfyUI-GGUF</strong> - For loading those quantized model files</li>
                    <li><strong>ComfyUI-Qwen-image-editing</strong> - For the Qwen-specific nodes that handle the vision-language stuff</li>
                </ul>

                <h2>The Workflow Breakdown</h2>
                <p>Here's where it gets interesting. Let me show you what each part of the workflow actually does.</p>

                <h3>1. Loading the Brain</h3>
                <p>First, you need to load three different models:</p>

                <pre><code><span class="code-comment"># UnetLoaderGGUF - The main diffusion model</span>
<span class="code-property">model_path</span>: <span class="code-string">"Qwen-Image-Edit-2509-Q8_0.gguf"</span>
<span class="code-property">output</span>: model

<span class="code-comment"># This is the actual neural network that generates images</span>
<span class="code-comment"># It's been quantized to 8-bit to fit in memory</span></code></pre>

                <p>The UNet is the core diffusion model. Think of it as the "artist" that will paint your edited image. But before it can do that, it needs to understand what you want.</p>

                <h3>2. The Vision Encoder</h3>
                <p>This is where Qwen2.5-VL comes in. It's a vision-language model that can look at your input image and understand what's in it:</p>

                <pre><code><span class="code-comment"># CLIPLoaderGGUF - Vision & text encoder</span>
<span class="code-property">clip_name</span>: <span class="code-string">"qwen2.5-vl-7b-it-q8_0.gguf"</span>
<span class="code-property">type</span>: <span class="code-string">"qwen_image"</span>
<span class="code-property">output</span>: clip</code></pre>

                <p>This encoder does two things simultaneously: it processes your text instruction (like "make the person smile") and analyzes your input image. Then it creates a joint understanding of both.</p>

                <h3>3. The Speed Hack</h3>
                <p>Normally, diffusion models take 20-50 steps to generate an image. Each step is a forward pass through the neural network, and on a Mac Mini, that's slow.</p>

                <p>Enter the Lightning LoRA:</p>

                <pre><code><span class="code-comment"># LoraLoaderModelOnly - Speed booster</span>
<span class="code-property">lora_name</span>: <span class="code-string">"Qwen-Image-Edit-2509-Lightning-4steps-V1.0-bf16.safetensors"</span>
<span class="code-property">strength</span>: <span class="code-number">1.0</span>
<span class="code-property">input</span>: model (from UNet loader)
<span class="code-property">output</span>: patched_model</code></pre>

                <p>LoRA stands for Low-Rank Adaptation. It's a technique for fine-tuning models without changing all the parameters. This specific LoRA has been trained to make the model converge in just 4 steps instead of 50. That's a 10x speedup.</p>

                <p>The trade-off? Slightly less photorealistic results, but for most edits, you won't notice the difference. And on a Mac Mini, going from 5 minutes to 30 seconds per image is absolutely worth it.</p>

                <h3>4. The VAE (The Unsung Hero)</h3>
                <p>VAE stands for Variational Autoencoder. It's the bridge between the pixel world and the latent world where the diffusion actually happens:</p>

                <pre><code><span class="code-comment"># VAELoader - Image encoder/decoder</span>
<span class="code-property">vae_name</span>: <span class="code-string">"qwen_image_vae.safetensors"</span>
<span class="code-property">output</span>: vae

<span class="code-comment"># Your 512x512 image gets compressed to 64x64x4 latent space</span>
<span class="code-comment"># That's a 64x reduction in size!</span>
<span class="code-comment"># The diffusion happens in this compressed space (much faster)</span>
<span class="code-comment"># Then gets decoded back to pixels at the end</span></code></pre>

                <p>This compression is crucial. Running diffusion on full-resolution pixels would be impossibly slow. The VAE lets us work in a compressed "thought space" where the model can be creative, then converts the result back to something you can actually see.</p>

                <h3>5. The Magic Node</h3>
                <p>This is where everything comes together. The TextEncodeQwenImageEditPlus node is what makes this whole workflow possible:</p>

                <pre><code><span class="code-comment"># TextEncodeQwenImageEditPlus - The conductor</span>
<span class="code-property">Inputs</span>:
  - clip (from CLIPLoaderGGUF)
  - vae (from VAELoader)
  - text: <span class="code-string">"Make the person smile"</span>
  - image1: your_input_image
  - image2: (optional)
  - image3: (optional)

<span class="code-property">Outputs</span>:
  - conditioning (guidance for the model)
  - reference_latents (features from your image)</code></pre>

                <p>This node does something remarkable. It takes your text instruction and your input image, and creates a "conditioning" that tells the UNet exactly what to generate. It's like giving an artist both a reference photo and a written brief.</p>

                <p>The reference latents are particularly cool - they're a compressed representation of your input image that the model uses as a starting point for the edit.</p>

                <h3>6. The Sampler (Where the Art Happens)</h3>
                <p>Finally, we get to the KSampler. This is where the actual image generation happens:</p>

                <pre><code><span class="code-comment"># KSampler - The artist at work</span>
<span class="code-property">steps</span>: <span class="code-number">4</span>           <span class="code-comment"># Thanks to Lightning LoRA!</span>
<span class="code-property">cfg</span>: <span class="code-number">1.0</span>          <span class="code-comment"># How closely to follow instructions</span>
<span class="code-property">sampler</span>: <span class="code-string">"euler"</span>    <span class="code-comment"># The algorithm for denoising</span>
<span class="code-property">denoise</span>: <span class="code-number">1.0</span>      <span class="code-comment"># Full replacement (complete edit)</span>

<span class="code-comment"># Inputs: model, conditioning, latent_image</span>
<span class="code-comment"># Output: noisy_latent that gradually becomes your image</span></code></pre>

                <p>The sampler starts with random noise and gradually shapes it into your desired image over 4 steps. Each step is guided by the conditioning from the previous node. It's like sculpting - starting from a block of marble (noise) and gradually revealing the statue (your edited image) inside.</p>

                <h3>7. Decoding the Result</h3>
                <p>Last step: convert the latent back to pixels you can see:</p>

                <pre><code><span class="code-comment"># VAEDecode - Back to pixel land</span>
<span class="code-property">samples</span>: (from KSampler)
<span class="code-property">vae</span>: (from VAELoader)
<span class="code-property">output</span>: your_edited_image.png</code></pre>

                <h2>The Results</h2>
                <p>So how well does it work? Surprisingly well! On my Mac Mini M2 Pro:</p>

                <ul>
                    <li><strong>768x768 images:</strong> ~15-20 seconds per edit</li>
                    <li><strong>512x512 images:</strong> ~8-12 seconds per edit</li>
                    <li><strong>Memory usage:</strong> ~12GB during inference</li>
                </ul>

                <p>The quality is impressive. The model understands context well - if you ask it to "change the background to a beach," it knows to keep the person in the foreground and only modify the background. It handles lighting consistency, shadows, and perspective better than I expected.</p>

                <h2>Adding More LoRAs</h2>
                <p>Want to customize the style? You can chain multiple LoRAs together. Here's how:</p>

                <pre><code><span class="code-comment"># Chain multiple LoRAs sequentially</span>

<span class="code-comment"># Step 1: Base model + Lightning</span>
UnetLoaderGGUF ‚Üí LoraLoaderModelOnly (Lightning) ‚Üí model_1

<span class="code-comment"># Step 2: Add style LoRA</span>
model_1 ‚Üí LoraLoaderModelOnly (Anime Style, strength: <span class="code-number">0.7</span>) ‚Üí model_2

<span class="code-comment"># Step 3: Add character consistency</span>
model_2 ‚Üí LoraLoaderModelOnly (Character LoRA, strength: <span class="code-number">0.5</span>) ‚Üí final_model

<span class="code-comment"># Use final_model in your KSampler</span></code></pre>

                <p>Order matters here. Apply the Lightning LoRA first (for speed), then style/character LoRAs. Each subsequent LoRA builds on the previous one.</p>

                <p>You can find LoRAs on <a href="https://civitai.com/" target="_blank">Civitai</a> or <a href="https://huggingface.co/" target="_blank">Hugging Face</a>. Search for "FLUX LoRA" or "SDXL LoRA" - they're compatible formats.</p>

                <h2>Pro Tips for macOS</h2>
                <p>A few things I learned the hard way:</p>

                <ol>
                    <li><strong>Use Metal Performance Shaders:</strong> In ComfyUI settings, set device to "mps". This routes inference through Apple's Metal backend instead of CPU.</li>
                    <li><strong>Close other apps:</strong> With 16GB unified memory, every MB counts. Close browsers, Slack, whatever you don't need.</li>
                    <li><strong>First run is slow:</strong> The model has to load into memory. Subsequent runs use the cached model and are much faster.</li>
                    <li><strong>Start small:</strong> Test with 512x512 images first. Once you're happy with the workflow, scale up.</li>
                </ol>

                <h2>Final Thoughts</h2>
                <p>Running AI image generation locally on a Mac Mini isn't just possible‚Äîit's actually practical. The GGUF format and Lightning LoRAs make it feasible, and ComfyUI makes it approachable.</p>

                <p>Sure, it's not as fast as cloud APIs, and you don't get the fancy inpainting UIs that Midjourney or DALL-E provide. But you get something more valuable: complete privacy, no usage limits, and the satisfaction of understanding exactly how the sausage is made.</p>

                <p>Plus, there's something deeply satisfying about watching your Mac Mini's fans spin up as it thinks about how to make a person smile. It's like having a tiny art studio in your computer.</p>

                <p>Give it a shot. Download the models, install ComfyUI, and start experimenting. The worst that happens is you learn something new about how these incredible models actually work under the hood.</p>

                <p>Happy editing! ü•î</p>
            </div>

            <div class="article-tags">
                <span class="tag">ComfyUI</span>
                <span class="tag">Qwen</span>
                <span class="tag">GGUF</span>
                <span class="tag">macOS</span>
                <span class="tag">AI</span>
                <span class="tag">Tutorial</span>
            </div>
        </article>
    </main>

    <footer class="footer">
        <div class="footer-content">
            <p class="footer-text">¬© 2026 Bahushruth CS</p>
            <div class="footer-social">
                <a href="https://x.com/Bahushruth" target="_blank" class="footer-link">X/Twitter</a>
                <a href="https://www.linkedin.com/in/bahushruth/" target="_blank" class="footer-link">LinkedIn</a>
                <a href="https://github.com/bahushruth" target="_blank" class="footer-link">GitHub</a>
            </div>
        </div>
    </footer>

    <script>
        // Add copy buttons to all code blocks
        document.querySelectorAll('pre').forEach(pre => {
            const button = document.createElement('button');
            button.className = 'copy-button';
            button.textContent = 'Copy';
            
            button.addEventListener('click', () => {
                const code = pre.querySelector('code');
                const text = code ? code.textContent : pre.textContent;
                
                navigator.clipboard.writeText(text).then(() => {
                    button.textContent = 'Copied!';
                    button.classList.add('copied');
                    
                    setTimeout(() => {
                        button.textContent = 'Copy';
                        button.classList.remove('copied');
                    }, 2000);
                });
            });
            
            pre.appendChild(button);
        });
    </script>
</body>
</html>