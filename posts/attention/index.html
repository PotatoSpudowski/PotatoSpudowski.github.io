<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding Transformers! (Part 1: Model Architecture) | GradientDissent</title><meta name=keywords content="Self-Attention,Transformers,NLP"><meta name=description content="In the past, we&rsquo;ve had different architectures for different modalities of data like CNNs for Images, RNNs for Text and GNNs for Graphs. Recently we have seen the adoption of Transformers for processing all types of data modalities. Transformers can be thought of like general purpose trainable architecture. Since the adoption of transformers has been growing so rapidly, It might be a good idea to revisit the original paper.
Introduction In sequence modelling and transduction problems, recurrent neural networks, and in particular extended short-term memory and gated recurrent neural networks, have become standard."><meta name=author content="Me"><link rel=canonical href=https://potatospudowski.github.io/posts/attention/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://potatospudowski.github.io/img/favicon_io/favicon-16x16.png><link rel=icon type=image/png sizes=16x16 href=https://potatospudowski.github.io/img/favicon_io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://potatospudowski.github.io/img/favicon_io/favicon-32x32.png><link rel=apple-touch-icon href=https://potatospudowski.github.io/img/favicon_io/apple-touch-icon.png><link rel=mask-icon href=https://potatospudowski.github.io/img/favicon_io/favicon-32x32.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Understanding Transformers! (Part 1: Model Architecture)"><meta property="og:description" content="In the past, we&rsquo;ve had different architectures for different modalities of data like CNNs for Images, RNNs for Text and GNNs for Graphs. Recently we have seen the adoption of Transformers for processing all types of data modalities. Transformers can be thought of like general purpose trainable architecture. Since the adoption of transformers has been growing so rapidly, It might be a good idea to revisit the original paper.
Introduction In sequence modelling and transduction problems, recurrent neural networks, and in particular extended short-term memory and gated recurrent neural networks, have become standard."><meta property="og:type" content="article"><meta property="og:url" content="https://potatospudowski.github.io/posts/attention/"><meta property="og:image" content="https://potatospudowski.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-10-31T15:16:19+05:30"><meta property="article:modified_time" content="2022-10-31T15:16:19+05:30"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://potatospudowski.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Understanding Transformers! (Part 1: Model Architecture)"><meta name=twitter:description content="In the past, we&rsquo;ve had different architectures for different modalities of data like CNNs for Images, RNNs for Text and GNNs for Graphs. Recently we have seen the adoption of Transformers for processing all types of data modalities. Transformers can be thought of like general purpose trainable architecture. Since the adoption of transformers has been growing so rapidly, It might be a good idea to revisit the original paper.
Introduction In sequence modelling and transduction problems, recurrent neural networks, and in particular extended short-term memory and gated recurrent neural networks, have become standard."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://potatospudowski.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Understanding Transformers! (Part 1: Model Architecture)","item":"https://potatospudowski.github.io/posts/attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Transformers! (Part 1: Model Architecture)","name":"Understanding Transformers! (Part 1: Model Architecture)","description":"In the past, we\u0026rsquo;ve had different architectures for different modalities of data like CNNs for Images, RNNs for Text and GNNs for Graphs. Recently we have seen the adoption of Transformers for processing all types of data modalities. Transformers can be thought of like general purpose trainable architecture. Since the adoption of transformers has been growing so rapidly, It might be a good idea to revisit the original paper.\nIntroduction In sequence modelling and transduction problems, recurrent neural networks, and in particular extended short-term memory and gated recurrent neural networks, have become standard.","keywords":["Self-Attention","Transformers","NLP"],"articleBody":"In the past, we’ve had different architectures for different modalities of data like CNNs for Images, RNNs for Text and GNNs for Graphs. Recently we have seen the adoption of Transformers for processing all types of data modalities. Transformers can be thought of like general purpose trainable architecture. Since the adoption of transformers has been growing so rapidly, It might be a good idea to revisit the original paper.\nIntroduction In sequence modelling and transduction problems, recurrent neural networks, and in particular extended short-term memory and gated recurrent neural networks, have become standard. Computational recurrence is often decomposed in recurrent models according to the symbol positions in the input and output sequences. Due to its intrinsic sequential nature, training examples cannot be parallelized. Recent work has improved computational efficiency significantly by employing factorization methods. However, the underlying barrier of sequential processing persists.\nAttention mechanisms have evolved to the point where they are now an indispensable component of compelling sequence modelling and transduction models in a variety of tasks. This has made it possible to model dependencies without taking into account the order in which they appear in the input or output sequences. However, in virtually every instance, such attention mechanisms are used in conjunction with a recurrent neural network\nIn order to create global relationships between input and output, the paper’s authors propose the Transformer, a model architecture that does not use recursion but rather relies solely on an attention mechanism. The Transformer can achieve a new state-of-the-art in translation quality and allows for substantially higher parallelization.\nBackground Reducing Sequential Computation in Deep Neural Networks The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S. All of these use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions. This makes it more difficult to learn dependencies between distant positions.\nSelf-Attention: An Intra-Attention Approach Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.\nHas self-attention been used successfully in a variety of tasks? A recurrent attention mechanism, as opposed to a sequence-aligned recurrence mechanism, is used as the foundation for end-to-end memory networks. It has been demonstrated that they are successful at answering questions in straightforward English and in language modelling activities.\nTransformer is the first transduction model that rely solely on self-attention to compute representations of its input and output.\nModel Architecture Auto-Regressive Neural Sequence Transduction Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps a sequence of symbol representations (x1,…,xn) to a sequence of continuous representations (z = z1,…,zn). When given z, the decoder then creates a sequence of symbols (y1,…, ym) one at a time. At each step, the model is auto-regressive, which means that it uses the symbols it has already made as input for the next step.\nFor both the encoder and the decoder, the Transformer has stacked self-attention and point-by-point layers that are all fully connected.\nclass EncoderDecoder(nn.Module): \"\"\" A standard Encoder-Decoder architecture. Base for this and many other models. \"\"\" def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \"Take in and process masked src and target sequences.\" return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) class Generator(nn.Module): \"Define standard linear + softmax generation step.\" def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return log_softmax(self.proj(x), dim=-1) Encoder and Decoder Stacks Encoder There are N = 6 layers in the encoder, each of which is identical in every way possible.\nThe layers are divided into two sublayers each.\nMulti-head self-attention mechanism, Simple, position-wise fully connected feed-forward network. We first normalise the layer and then use a residual connection around each of the two sublayers. LayerNorm(x + Sublayer(x)) is the output of a sub-layer, where Sublayer(x) is the function implemented by that sub-layer. All model sublayers and embedding layers generate 512-dimensional outputs to facilitate these residual connections.\ndef clones(module, N): return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) class Encoder(nn.Module): def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): for layer in self.layers: x = layer(x, mask) return self.norm(x) class LayerNorm(nn.Module): \"\"\" Layer Norm module LayerNorm is a type of normalization that is applied to the output of each sub-layer in the encoder. This normalization helps to improve the stability of the AI model and makes it easier for the model to learn. Layer normalisation tries to diminish the impact of covariant shift. In other words, it prevents the mean and standard deviation of embedding vector elements from shifting, which renders training unstable and sluggish. \"\"\" def __init__(self, features, eps=1e-6): super(LayerNorm, self).__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2 = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 class EncoderLayer(nn.Module): def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward) Decoder In the same way that the encoder is made up of N = 6 levels, the decoder is also built up of identical layers. In order to execute multi-head attention on the encoder stack’s output, the decoder adds a third sub-layer on top of the original two in each encoder layer. We use residual connections around each of the sub-layers, much like the encoder does, and then we normalise the layers. Moreover, we tweak the decoder stack’s self-attention sub-layer to make positions uninterested in focusing on those that come after them. This masking, in conjunction with the output embeddings’ positional offset of one, ensures that the predictions for position i can only depend on the known outputs at positions less than i.\nWhen we talk about masked multi-head attention, this indicates that the multi-head attention receives inputs that are disguised such that the attention mechanism does not use any of the information from the positions that are hidden. The researchers applied the mask within the attention computation, according to the study, and they mention that they did so by setting attention scores to a value greater than negative infinity (or a very large negative number). Masked places are given an effective probability of zero thanks to the softmax function found within the attention systems.\n(1, 0, 0, 0, 0, …, 0) =\u003e () (1, 1, 0, 0, 0, …, 0) =\u003e (, ‘Friday’) (1, 1, 1, 0, 0, …, 0) =\u003e (, ‘Friday’, ‘hai’) (1, 1, 1, 1, 0, …, 0) =\u003e (, ‘Friday’, ‘hai’, 'pencho') (1, 1, 1, 1, 1, …, 0) =\u003e (, 'Friday', 'hai', 'pencho', '!') Another type of multi-head attention, the source-target attention, determines the attention values between the features (embeddings) of the input sentence and the features of the output (still partial) sentence. It does this by calculating the distance between the two sets of features.\nclass Decoder(nn.Module): def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x) class DecoderLayer(nn.Module): def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) Position-wise Feed-Forward Networks Encoder and decoder layers both have attention sub-layers and a fully connected feed-forward network, which are applied independently and uniformly to each position. This is made up of two linear transformations separated by a ReLU activation.\n$$FFN(x) = max(0, xW_1+b_1)W_2 + b_2$$\nDespite the fact that the linear transformations are identical across all places, their parameters vary from layer to layer. One other approach to explain this would be to say that it consists of two convolutions with a kernel size of 1. The dimensionality of input and output is\n$$d_{model} = 512$$\nand the inner-layer has dimensionality\n$$d_{ff} = 2048$$\nclass PositionwiseFeedForward(nn.Module): \"\"\" Multiplying x by W1 doubles its size to 2048, then dividing it by W2 brings it back down to 512. In FFN, the weights for all positions inside the same layer are the same. \"\"\" def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(self.w_1(x).relu())) Embeddings and Softmax In a manner analogous to that of previous sequence transduction models, authors make use of learnt embeddings in order to transform input tokens and output tokens into vectors with the dimension.\n$$d_{model}$$\nThey convert the decoder output into projected next-token probabilities by employing the standard learnt linear transformation in conjunction with the softmax function. Within the model, they make use of the same weight matrix for both of the embedding layers as well as the pre-softmax linear transformation. Within the embedding layers, they multiply those weights by the square root of the model’s dimension.\n$$\\sqrt{d_{model}}$$ ​\nclass Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) Positional Encoding I’ve saved my favorite for last. When and why do we require positional encoding?\nA non-recurrent architecture of multi-head attention, Transformer employs positional encoding to provide the order context. When recurrent neural networks are given sequence inputs, the input itself defines the sequential order (ordering of time-steps). But the Transformer’s Multi-Head Attention layer is a feed-forward layer, which takes in the entire sequence at once instead of sequentially step by step.. Attention is sequence-independent since it is computed on each datapoint (time-step) independently, hence it does not take into account the ordering of datapoints.\nThe principle of positional encoding is employed to solve this issue. Simply put, this entails adding a tensor to the input sequence that has the desired characteristics. To do this, “positional encodings” are added to the input embeddings at the base of the encoder and decoder stacks. Positional encodings and embeddings are of the same dimension, allowing for a simple addition of the two.\nThey employ sine and cosine functions with varying frequencies:\n$$ PE_{(pos, 2i)} = sin(pos/1000^{2i/d_{model}}) $$ $$ PE_{(pos, 2{i+1})} = cos(pos/1000^{2i/d_{model}}) $$\nwhere pos is the location and i represents the size. That is, a sinusoid represents one dimension of the positional encoding. The wavelengths increase geometrically from 2 to 100002. Authors opted for this function on the assumption that it would facilitate the model’s ability to pick up on relative positional cues, since for every fixed offset k,\n$$ PE_{pos+k} = \\textrm{linear \\ function \\ of } PE_{pos+k} $$\nThe total embedding and positional encoding sums in the encoder and decoder stacks are also subjected to dropout. The starting point for the base model is a rate of\n$$P_{drop} = 0.1$$ ​\nclass PositionalEncoding(nn.Module): def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp( torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model) ) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer(\"pe\", pe) def forward(self, x): x = x + self.pe[:, : x.size(1)].requires_grad_(False) return self.dropout(x) Thats all for now 🤭\nIn the next part, We will try to understand the attention mechanism in transformers! I felt like it would be better to have a separate post instead of writing about it here.\nReferences Attention Is All You Need https://arxiv.org/abs/1706.03762 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nThe Annotated Transformer http://nlp.seas.harvard.edu/2018/04/03/attention.html Harvard NLP\nThe Illustrated Transformer http://jalammar.github.io/illustrated-transformer/ Jay Alammar\nTransformer break-down: Positional Encoding https://medium.datadriveninvestor.com/transformer-break-down-positional-encoding-c8d1bbbf79a8\n","wordCount":"2033","inLanguage":"en","datePublished":"2022-10-31T15:16:19+05:30","dateModified":"2022-10-31T15:16:19+05:30","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://potatospudowski.github.io/posts/attention/"},"publisher":{"@type":"Organization","name":"GradientDissent","logo":{"@type":"ImageObject","url":"https://potatospudowski.github.io/img/favicon_io/favicon-16x16.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://potatospudowski.github.io/ accesskey=h title="GradientDissent (Alt + H)"><img src=https://potatospudowski.github.io/apple-touch-icon.png alt aria-label=logo height=35>GradientDissent</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://potatospudowski.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=https://potatospudowski.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://potatospudowski.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://potatospudowski.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://potatospudowski.github.io/posts/>Posts</a></div><h1 class=post-title>Understanding Transformers! (Part 1: Model Architecture)</h1><div class=post-meta><span title='2022-10-31 15:16:19 +0530 IST'>October 31, 2022</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;2033 words&nbsp;·&nbsp;Me</div></header><div class=post-content><p>In the past, we&rsquo;ve had different architectures for different modalities of data like CNNs for Images, RNNs for Text and GNNs for Graphs. Recently we have seen the adoption of Transformers for processing all types of data modalities. Transformers can be thought of like general purpose trainable architecture. Since the adoption of transformers has been growing so rapidly, It might be a good idea to revisit the <a href=https://arxiv.org/abs/1706.03762>original paper</a>.</p><p><img loading=lazy src=/img/transformers/1.jpg#center alt=text></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>In sequence modelling and transduction problems, recurrent neural networks, and in particular extended short-term memory and gated recurrent neural networks, have become standard. Computational recurrence is often decomposed in recurrent models according to the symbol positions in the input and output sequences. Due to its intrinsic sequential nature, training examples cannot be parallelized. Recent work has improved computational efficiency significantly by employing factorization methods. However, the underlying barrier of sequential processing persists.</p><p><img loading=lazy src=/img/transformers/2.jpeg#center alt=text></p><p>Attention mechanisms have evolved to the point where they are now an indispensable component of compelling sequence modelling and transduction models in a variety of tasks. This has made it possible to model dependencies without taking into account the order in which they appear in the input or output sequences. However, in virtually every instance, such attention mechanisms are used in conjunction with a recurrent neural network</p><p>In order to create global relationships between input and output, the paper&rsquo;s authors propose the Transformer, a model architecture that does not use recursion but rather relies solely on an attention mechanism. The Transformer can achieve a new state-of-the-art in translation quality and allows for substantially higher parallelization.</p><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><h4 id=reducing-sequential-computation-in-deep-neural-networks>Reducing Sequential Computation in Deep Neural Networks<a hidden class=anchor aria-hidden=true href=#reducing-sequential-computation-in-deep-neural-networks>#</a></h4><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S. All of these use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions. This makes it more difficult to learn dependencies between distant positions.</p><h4 id=self-attention-an-intra-attention-approach>Self-Attention: An Intra-Attention Approach<a hidden class=anchor aria-hidden=true href=#self-attention-an-intra-attention-approach>#</a></h4><p>Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.</p><h4 id=has-self-attention-been-used-successfully-in-a-variety-of-tasks>Has self-attention been used successfully in a variety of tasks?<a hidden class=anchor aria-hidden=true href=#has-self-attention-been-used-successfully-in-a-variety-of-tasks>#</a></h4><p>A recurrent attention mechanism, as opposed to a sequence-aligned recurrence mechanism, is used as the foundation for end-to-end memory networks. It has been demonstrated that they are successful at answering questions in straightforward English and in language modelling activities.</p><p>Transformer is the first transduction model that rely solely on self-attention to compute representations of its input and output.</p><h2 id=model-architecture>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h2><h4 id=auto-regressive-neural-sequence-transduction>Auto-Regressive Neural Sequence Transduction<a hidden class=anchor aria-hidden=true href=#auto-regressive-neural-sequence-transduction>#</a></h4><p>Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps a sequence of symbol representations (x1,&mldr;,xn) to a sequence of continuous representations (z = z1,&mldr;,zn). When given z, the decoder then creates a sequence of symbols (y1,&mldr;, ym) one at a time. At each step, the model is auto-regressive, which means that it uses the symbols it has already made as input for the next step.</p><p>For both the encoder and the decoder, the Transformer has stacked self-attention and point-by-point layers that are all fully connected.</p><p><img loading=lazy src=/img/transformers/3.png#center alt=text></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>EncoderDecoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    A standard Encoder-Decoder architecture. Base for this and many
</span></span></span><span class=line><span class=cl><span class=s2>    other models.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>encoder</span><span class=p>,</span> <span class=n>decoder</span><span class=p>,</span> <span class=n>src_embed</span><span class=p>,</span> <span class=n>tgt_embed</span><span class=p>,</span> <span class=n>generator</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>EncoderDecoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>encoder</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>decoder</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>src_embed</span> <span class=o>=</span> <span class=n>src_embed</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>tgt_embed</span> <span class=o>=</span> <span class=n>tgt_embed</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>generator</span> <span class=o>=</span> <span class=n>generator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;Take in and process masked src and target sequences.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>),</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>src_embed</span><span class=p>(</span><span class=n>src</span><span class=p>),</span> <span class=n>src_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tgt_embed</span><span class=p>(</span><span class=n>tgt</span><span class=p>),</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Generator</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Define standard linear + softmax generation step.&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>vocab</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Generator</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>vocab</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>log_softmax</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>proj</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=encoder-and-decoder-stacks>Encoder and Decoder Stacks<a hidden class=anchor aria-hidden=true href=#encoder-and-decoder-stacks>#</a></h3><h4 id=encoder>Encoder<a hidden class=anchor aria-hidden=true href=#encoder>#</a></h4><p>There are N = 6 layers in the encoder, each of which is identical in every way possible.</p><p>The layers are divided into two sublayers each.</p><ol><li>Multi-head self-attention mechanism,</li><li>Simple, position-wise fully connected feed-forward network.</li></ol><p>We first normalise the layer and then use a residual connection around each of the two sublayers. <strong>LayerNorm(x + Sublayer(x))</strong> is the output of a sub-layer, where <strong>Sublayer(x)</strong> is the function implemented by that sub-layer. All model sublayers and embedding layers generate 512-dimensional outputs to facilitate these residual connections.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>clones</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>N</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>copy</span><span class=o>.</span><span class=n>deepcopy</span><span class=p>(</span><span class=n>module</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>N</span><span class=p>)])</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Encoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>layer</span><span class=p>,</span> <span class=n>N</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Encoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=n>N</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>layer</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>LayerNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Layer Norm module
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    LayerNorm is a type of normalization that is applied to the output of each sub-layer in the encoder. 
</span></span></span><span class=line><span class=cl><span class=s2>    This normalization helps to improve the stability of the AI model and makes it easier for the model to learn. 
</span></span></span><span class=line><span class=cl><span class=s2>    Layer normalisation tries to diminish the impact of covariant shift. 
</span></span></span><span class=line><span class=cl><span class=s2>    In other words, it prevents the mean and standard deviation of embedding vector elements from shifting, 
</span></span></span><span class=line><span class=cl><span class=s2>    which renders training unstable and sluggish.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>features</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>LayerNorm</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>a_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>features</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>b_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>features</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=n>eps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mean</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>std</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>a_2</span> <span class=o>*</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=n>std</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>b_2</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>EncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>self_attn</span><span class=p>,</span> <span class=n>feed_forward</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>EncoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>self_attn</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>feed_forward</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>SublayerConnection</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>),</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>size</span> <span class=o>=</span> <span class=n>size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>0</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>1</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=/img/transformers/4.png#center alt=text></p><h4 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h4><p>In the same way that the encoder is made up of N = 6 levels, the decoder is also built up of identical layers. <strong>In order to execute multi-head attention on the encoder stack&rsquo;s output, the decoder adds a third sub-layer on top of the original two in each encoder layer</strong>. We use residual connections around each of the sub-layers, much like the encoder does, and then we normalise the layers. Moreover, we tweak the decoder stack&rsquo;s self-attention sub-layer to make positions uninterested in focusing on those that come after them. This masking, in conjunction with the output embeddings&rsquo; positional offset of one, ensures that the predictions for position <strong>i</strong> can only depend on the known outputs at positions less than <strong>i</strong>.</p><p>When we talk about <strong>masked multi-head attention</strong>, this indicates that the multi-head attention receives inputs that are disguised such that the attention mechanism does not use any of the information from the positions that are hidden. The researchers applied the mask within the attention computation, according to the study, and they mention that they did so by setting attention scores to a value greater than negative infinity (or a very large negative number). Masked places are given an effective probability of zero thanks to the softmax function found within the attention systems.</p><pre tabindex=0><code>(1, 0, 0, 0, 0, …, 0) =&gt; (&lt;SOS&gt;)
(1, 1, 0, 0, 0, …, 0) =&gt; (&lt;SOS&gt;, ‘Friday’)
(1, 1, 1, 0, 0, …, 0) =&gt; (&lt;SOS&gt;, ‘Friday’, ‘hai’)
(1, 1, 1, 1, 0, …, 0) =&gt; (&lt;SOS&gt;, ‘Friday’, ‘hai’, &#39;pencho&#39;)
(1, 1, 1, 1, 1, …, 0) =&gt; (&lt;SOS&gt;, &#39;Friday&#39;, &#39;hai&#39;, &#39;pencho&#39;, &#39;!&#39;)
</code></pre><p>Another type of multi-head attention, the <strong>source-target attention</strong>, determines the attention values between the features (embeddings) of the input sentence and the features of the output (still partial) sentence. It does this by calculating the distance between the two sets of features.</p><p><img loading=lazy src=/img/transformers/6.jpg#center alt=text></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Decoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>layer</span><span class=p>,</span> <span class=n>N</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Decoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=n>N</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>layer</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>DecoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>self_attn</span><span class=p>,</span> <span class=n>src_attn</span><span class=p>,</span> <span class=n>feed_forward</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>DecoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>size</span> <span class=o>=</span> <span class=n>size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>self_attn</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>src_attn</span> <span class=o>=</span> <span class=n>src_attn</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>feed_forward</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>SublayerConnection</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>),</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>m</span> <span class=o>=</span> <span class=n>memory</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>0</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>1</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>src_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>m</span><span class=p>,</span> <span class=n>m</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>2</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>)</span>
</span></span></code></pre></div><p><img loading=lazy src=/img/transformers/5.png#center alt=text></p><h3 id=position-wise-feed-forward-networks>Position-wise Feed-Forward Networks<a hidden class=anchor aria-hidden=true href=#position-wise-feed-forward-networks>#</a></h3><p>Encoder and decoder layers both have attention sub-layers and a fully connected feed-forward network, which are applied independently and uniformly to each position. This is made up of two linear transformations separated by a ReLU activation.</p><p><img loading=lazy src=/img/transformers/7.jpg#center alt=text></p><p>$$FFN(x) = max(0, xW_1+b_1)W_2 + b_2$$</p><p>Despite the fact that the linear transformations are identical across all places, their parameters vary from layer to layer. One other approach to explain this would be to say that it consists of two convolutions with a kernel size of 1. The dimensionality of input and output is</p><p>$$d_{model} = 512$$</p><p>and the inner-layer has dimensionality</p><p>$$d_{ff} = 2048$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionwiseFeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Multiplying x by W1 doubles its size to 2048, then dividing it by W2 brings it back down to 512. 
</span></span></span><span class=line><span class=cl><span class=s2>    In FFN, the weights for all positions inside the same layer are the same.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>PositionwiseFeedForward</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>w_1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>w_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_ff</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>w_2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>w_1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>relu</span><span class=p>()))</span>
</span></span></code></pre></div><h3 id=embeddings-and-softmax>Embeddings and Softmax<a hidden class=anchor aria-hidden=true href=#embeddings-and-softmax>#</a></h3><p>In a manner analogous to that of previous sequence transduction models, authors make use of learnt embeddings in order to transform input tokens and output tokens into vectors with the dimension.</p><p>$$d_{model}$$</p><p>They convert the decoder output into projected next-token probabilities by employing the standard learnt linear transformation in conjunction with the softmax function. Within the model, they make use of the same weight matrix for both of the embedding layers as well as the pre-softmax linear transformation. Within the embedding layers, they multiply those weights by the square root of the model&rsquo;s dimension.</p><p>$$\sqrt{d_{model}}$$
​</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Embeddings</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>vocab</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Embeddings</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lut</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>lut</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>*</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=positional-encoding>Positional Encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h3><p>I&rsquo;ve saved my favorite for last. When and why do we require positional encoding?</p><p>A non-recurrent architecture of multi-head attention, Transformer employs positional encoding to provide the order context. When recurrent neural networks are given sequence inputs, the input itself defines the sequential order (ordering of time-steps). But the Transformer&rsquo;s Multi-Head Attention layer is a feed-forward layer, which takes in the entire sequence at once instead of sequentially step by step.. Attention is sequence-independent since it is computed on each datapoint (time-step) independently, hence it does not take into account the ordering of datapoints.</p><p><img loading=lazy src=/img/transformers/8.jpg#center alt=text></p><p>The principle of positional encoding is employed to solve this issue. Simply put, this entails adding a tensor to the input sequence  that has the desired characteristics. </p><p>To do this, &ldquo;positional encodings&rdquo; are added to the input embeddings at the base of the encoder and decoder stacks. Positional encodings and embeddings are of the same dimension, allowing for a simple addition of the two.</p><p>They employ sine and cosine functions with varying frequencies:</p><p>$$ PE_{(pos, 2i)} = sin(pos/1000^{2i/d_{model}}) $$
$$ PE_{(pos, 2{i+1})} = cos(pos/1000^{2i/d_{model}}) $$</p><p>where <strong>pos</strong> is the location and <strong>i</strong> represents the size. That is, a sinusoid represents one dimension of the positional encoding. The wavelengths increase geometrically from <strong>2</strong> to <strong>100002</strong>. Authors opted for this function on the assumption that it would facilitate the model&rsquo;s ability to pick up on relative positional cues, since for every fixed offset <strong>k</strong>,</p><p>$$ PE_{pos+k} = \textrm{linear \ function \ of } PE_{pos+k} $$</p><p>The total embedding and positional encoding sums in the encoder and decoder stacks are also subjected to dropout. The starting point for the base model is a rate of</p><p>$$P_{drop} = 0.1$$
​</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionalEncoding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=n>max_len</span><span class=o>=</span><span class=mi>5000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>PositionalEncoding</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Compute the positional encodings once in log space.</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>position</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>max_len</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>div_term</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>*</span> <span class=o>-</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>10000.0</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>pe</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>&#34;pe&#34;</span><span class=p>,</span> <span class=n>pe</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pe</span><span class=p>[:,</span> <span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)]</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>(</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><p>Thats all for now 🤭</p><p>In the next part, We will try to understand the attention mechanism in transformers!
I felt like it would be better to have a separate post instead of writing about it here.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><p>Attention Is All You Need
<a href=https://arxiv.org/abs/1706.03762>https://arxiv.org/abs/1706.03762</a>
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</p></li><li><p>The Annotated Transformer
<a href=http://nlp.seas.harvard.edu/2018/04/03/attention.html>http://nlp.seas.harvard.edu/2018/04/03/attention.html</a>
Harvard NLP</p></li><li><p>The Illustrated Transformer
<a href=http://jalammar.github.io/illustrated-transformer/>http://jalammar.github.io/illustrated-transformer/</a>
Jay Alammar</p></li><li><p>Transformer break-down: Positional Encoding
<a href=https://medium.datadriveninvestor.com/transformer-break-down-positional-encoding-c8d1bbbf79a8>https://medium.datadriveninvestor.com/transformer-break-down-positional-encoding-c8d1bbbf79a8</a></p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://potatospudowski.github.io/tags/self-attention/>Self-Attention</a></li><li><a href=https://potatospudowski.github.io/tags/transformers/>Transformers</a></li><li><a href=https://potatospudowski.github.io/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=next href=https://potatospudowski.github.io/posts/gnn/><span class=title>Next »</span><br><span>Understanding the mathematical foundations and applications of Graph Convolutional Networks</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformers! (Part 1: Model Architecture) on twitter" href="https://twitter.com/intent/tweet/?text=Understanding%20Transformers%21%20%28Part%201%3a%20Model%20Architecture%29&url=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fattention%2f&hashtags=Self-Attention%2cTransformers%2cNLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformers! (Part 1: Model Architecture) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fattention%2f&title=Understanding%20Transformers%21%20%28Part%201%3a%20Model%20Architecture%29&summary=Understanding%20Transformers%21%20%28Part%201%3a%20Model%20Architecture%29&source=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fattention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformers! (Part 1: Model Architecture) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fattention%2f&title=Understanding%20Transformers%21%20%28Part%201%3a%20Model%20Architecture%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformers! (Part 1: Model Architecture) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fattention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformers! (Part 1: Model Architecture) on whatsapp" href="https://api.whatsapp.com/send?text=Understanding%20Transformers%21%20%28Part%201%3a%20Model%20Architecture%29%20-%20https%3a%2f%2fpotatospudowski.github.io%2fposts%2fattention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformers! (Part 1: Model Architecture) on telegram" href="https://telegram.me/share/url?text=Understanding%20Transformers%21%20%28Part%201%3a%20Model%20Architecture%29&url=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fattention%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://potatospudowski.github.io/>GradientDissent</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>