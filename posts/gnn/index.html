<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding the mathematical foundations and applications of Graph Convolutional Networks | GradientDissent</title><meta name=keywords content="Graph Convolutional Networks,Graph ML"><meta name=description content="Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.
Deep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?"><meta name=author content="Me"><link rel=canonical href=https://potatospudowski.github.io/posts/gnn/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://potatospudowski.github.io/img/favicon_io/favicon-16x16.png><link rel=icon type=image/png sizes=16x16 href=https://potatospudowski.github.io/img/favicon_io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://potatospudowski.github.io/img/favicon_io/favicon-32x32.png><link rel=apple-touch-icon href=https://potatospudowski.github.io/img/favicon_io/apple-touch-icon.png><link rel=mask-icon href=https://potatospudowski.github.io/img/favicon_io/favicon-32x32.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Understanding the mathematical foundations and applications of Graph Convolutional Networks"><meta property="og:description" content="Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.
Deep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?"><meta property="og:type" content="article"><meta property="og:url" content="https://potatospudowski.github.io/posts/gnn/"><meta property="og:image" content="https://potatospudowski.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-06-19T00:41:42+05:30"><meta property="article:modified_time" content="2021-06-19T00:41:42+05:30"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://potatospudowski.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Understanding the mathematical foundations and applications of Graph Convolutional Networks"><meta name=twitter:description content="Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.
Deep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://potatospudowski.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Understanding the mathematical foundations and applications of Graph Convolutional Networks","item":"https://potatospudowski.github.io/posts/gnn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding the mathematical foundations and applications of Graph Convolutional Networks","name":"Understanding the mathematical foundations and applications of Graph Convolutional Networks","description":"Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.\nDeep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?","keywords":["Graph Convolutional Networks","Graph ML"],"articleBody":"Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.\nDeep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?\nThus, Graph ML comes into play. In this blog, I attempt to explain why graph convolutional networks were developed and their underlying mathematics.\nWhat are networks/graphs? Networks are a language for expressing large systems comprised of interacting components. Graphs can be used to display a wide variety of data kinds.\nSocial Networks, Communication Networks, Network of Neurons, Molecules, and Event Graphs are examples of networks.\nA graph is a two-component data structure in computer science, consisting of nodes (vertices) and edges. G = (V, E), where V is the collection of nodes and E are the edges between them, defines a graph.\nEdges are directed if there are directional dependencies between nodes. Otherwise, edges are undirected.\nGraphs are frequently represented by the adjacency matrix A. If a graph has n nodes, A has (n x n) dimensions. Occasionally, nodes possess a set of features. If the node has f features, then the dimension of the node feature matrix X is (n x f).\nWhy is graph analysis difficult? The complexity of graph data has presented numerous hurdles for conventional machine learning systems.\nConventional Machine Learning and Deep Learning tools focus in simple data types. As images with the same size and structure, which can be compared to fixed-size grid graphs. Text and voice are sequences, hence they can be compared to line graphs.\nThere are, however, more sophisticated networks without a fixed structure, with variable-sized, unordered nodes that can have varying numbers of neighbours. Existing machine learning methods have the fundamental assumption that instances are independent of one another. This is untrue for graph data, as each node is connected to others by various forms of connections.\nIntroduction to graph representation learning Embedding nodes is a basic notion in graph theory. This is accomplished by assigning a weight to each node in the graph and then mapping them to a d-dimensional embedding space (a low-dimensional space rather than the real dimension of the graph) so that nodes with similar properties are embedded close to one another.\nEmbedding nodes in the network Define an encoder Define a node similarity function Optimize the parameters of the encoder so that: Encoder: Maps a node to a low-dimensional vector\n$$Enc(v) = Zv $$\nSimilarity function: Specifies how the relationships in vector space map to the relationships in the original network\n$$similarity(u,v) \\approx Z_u ^ \\top Z_v $$\nShallow encoders vs Deep graph encoders Graphs can be represented in using 2 techniques. Shallow encoders and Deep encoders.\n“Shallow” Encoding “Shallow” encoding is the simplest way to encode. It means that the encoder is just an embedding-lookup, and it could be written as:\n$$Enc(v) = Zv $$\nEach column in Z represents an embedding of a node, and the number of rows in Z is equal to the size of the embeddings. v is the indicator vector that points to node v. It has all zeros except for a single one in column v. In “shallow” encoding, we can see that each node is given a unique embedding vector.\n“Shallow Encoders” Have Their Limits Since each node has its own embedding, Shallow Encoders don’t scale. Shallow Encoders are transductive by nature. It can only make embeddings for a single graph that stays the same. Also the features of nodes are not taken into account.\nDifferent types of shallow encoders:\nRandom walk Node2vec TransE “Deep” Encoding The current state of the art performance is achieved using Deep graph encoders.\n$$Enc(v) = Multiple\\space layers\\space of\\space non\\space linear \\space transforms $$\nGraph Convolutional Networks (GCN) GCNs are a type of deep graph encoders. The majority of neural networks operate on graphs of fixed size, such as images and texts. However, in the actual world, graph sizes are not fixed. Let’s have a look at the computational graphs of GCNs.\nGiven a graph G = (V,A,X) such that:\nV is the vertex set A is the adjacency matrix X ∈ R^(m×|V|) is the node feature matrix The computational graph must preserve the structure of graph G and incorporate the neighboring characteristics of the nodes. For instance, the embedding vector of node A should consist of characteristics of its neighbors B, C, and D without regard to their order. One technique to accomplish this is by averaging the characteristics of B,C,D.\nWith two layers, the computational graph of G will appear as follows:\nThe computational graph for node A:\nAnalogy between image convolution and graph convolution CNNs and GCNs have a number of operational commonalities.\nIn a CNN Conv layer, a window moves across a matrix and aggregates the local representations before proceeding to the next stride. Now, when considering the Graph Conv layer, the representations of nodes are created by aggregating the representations of adjacent nodes. Comparable to the adjacent pixels in a CNN.\nGraph convolutional layer and propagation rule Every neural network layer can then be written as a nonlinear function\n$$H^{(l+1)} = f(H^{(l)},A) $$\nThe propagation rule: $$f(H^{(l)},A) = \\sigma (\\hat{D}^{-{1 \\over 2}}\\hat{A}\\hat{D}^{-{1 \\over 2}}H^{(l)}W^{(l)})$$\nLet’s try to understand this equation using code 1. Imports import numpy as np import networkx as nx 2. The adjacency matrix A = np.matrix([ [0, 1, 0, 0], [0, 0, 1, 1], [0, 1, 0, 0], [1, 0, 0, 0]], dtype=float ) G = nx.from_numpy_matrix(A) This is how the graph looks like\n3. The feature matrix a = [1,0] b = [0,1] X = np.matrix([a,b,a,b]) #features print(X) #matrix([[1, 0], # [0, 1], # [1, 0], # [0, 1]]) 4. Propagation rule Multiplying Adjacency and Feature matrix\nprint(A @ X) #matrix([[0., 1.], # [1., 1.], # [0., 1.], # [1., 0.]]) # a b #a 0 1 a is connected to 0 a and 1 b (Node 0) #b 1 1 b is connected to 1 a and 1 b (Node 1) #a 0 1 a is connected to 0 a and 1 b (Node 2) #b 1 0 b is connected to 1 a and 0 b (Node 3) #The aggregated representation of a node does not include its own features! Creating an Identity matrix\nI = np.matrix(np.eye(A.shape[0])) print(I) # matrix([[1., 0., 0., 0.], # [0., 1., 0., 0.], # [0., 0., 1., 0.], # [0., 0., 0., 1.]]) Getting the projection matrix\nA_hat = A + I print(A_hat) # matrix([[1., 1., 0., 0.], # [0., 1., 1., 1.], # [0., 1., 1., 0.], # [1., 0., 0., 1.]]) Multiplying the projection matrix with feature matrix\nprint(A_hat @ X) # matrix([[1., 1.], # [1., 2.], # [1., 1.], # [1., 1.]]) # a b #a 1 1 a is connected to 1 a and 1 b (Node 0) #b 1 2 b is connected to 1 a and 2 b (Node 1) #a 1 1 a is connected to 1 a and 1 b (Node 2) #b 1 1 b is connected to 1 a and 1 b (Node 3) #The aggregated representation of a node does include its own features! Yayy now the representation of nodes include themselves!!!\nNow normalizing the representations\nD_hat = np.array(np.sum(A_hat, axis=0))[0] D_hat = np.matrix(np.diag(D_hat)) print(D_hat) # matrix([[2., 0., 0., 0.], # [0., 3., 0., 0.], # [0., 0., 2., 0.], # [0., 0., 0., 2.]]) print(D_hat**-1 @ A_hat @ X) # matrix([[0.5 , 0.5 ], # [0.33333333, 0.66666667], # [0.5 , 0.5 ], # [0.5 , 0.5 ]]) # Result, #matrix([[1., 1.], # [1., 2.], # [1., 1.], # [1., 1.]]) # is normalized to #matrix([[0.5 , 0.5 ], # [0.33333333, 0.66666667], # [0.5 , 0.5 ], # [0.5 , 0.5 ]]) A better way to normalize\nD_hat_inv = np.sqrt(D_hat**-1) H = D_hat_inv @ A_hat @ D_hat_inv @ X print(H) # matrix([[0.5 , 0.40824829], # [0.40824829, 0.74158162], # [0.5 , 0.40824829], # [0.5 , 0.5 ]]) Assigning weights\n#Weights np.random.seed(0) W = 2*np.random.rand(2,5) - 1 W = np.matrix(W) print(W) # matrix([[ 0.09762701, 0.43037873, 0.20552675, 0.08976637, -0.1526904 ], # [ 0.29178823, -0.12482558, 0.783546 , 0.92732552, -0.23311696]]) Finding the outputs of the hidden layer\nZ = H @ W print(Z) # matrix([[ 0.16793555, 0.16422954, 0.42264469, 0.42346224, -0.1715148 ], # [ 0.25624085, 0.08313303, 0.66496926, 0.72433453, -0.23521085], # [ 0.16793555, 0.16422954, 0.42264469, 0.42346224, -0.1715148 ], # [ 0.19470762, 0.15277658, 0.49453638, 0.50854594, -0.19290368]]) That is the propagation rule!\n4. Building the Graph Conv layer def GraphConv_layer(A_hat, D_hat, X, W): D_hat_inv = np.sqrt(D_hat**-1) return (D_hat_inv @ A_hat @ D_hat_inv @ X @ W) Which is $$f(H^{(l)},A) = \\sigma (\\hat{D}^{-{1 \\over 2}}\\hat{A}\\hat{D}^{-{1 \\over 2}}H^{(l)}W^{(l)})$$\nReferences CS224W: Machine Learning with Graphs: https://web.stanford.edu/class/cs224w/ Semi-Supervised Classification with Graph Convolutional Networks: https://tkipf.github.io/graph-convolutional-networks/ Embedding molecules using GCNs ","wordCount":"1496","inLanguage":"en","datePublished":"2021-06-19T00:41:42+05:30","dateModified":"2021-06-19T00:41:42+05:30","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://potatospudowski.github.io/posts/gnn/"},"publisher":{"@type":"Organization","name":"GradientDissent","logo":{"@type":"ImageObject","url":"https://potatospudowski.github.io/img/favicon_io/favicon-16x16.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://potatospudowski.github.io/ accesskey=h title="GradientDissent (Alt + H)"><img src=https://potatospudowski.github.io/apple-touch-icon.png alt aria-label=logo height=35>GradientDissent</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://potatospudowski.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=https://potatospudowski.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://potatospudowski.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://potatospudowski.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://potatospudowski.github.io/posts/>Posts</a></div><h1 class=post-title>Understanding the mathematical foundations and applications of Graph Convolutional Networks</h1><div class=post-meta><span title='2021-06-19 00:41:42 +0530 IST'>June 19, 2021</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1496 words&nbsp;·&nbsp;Me</div></header><div class=post-content><p>Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.</p><p>Deep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?</p><p>Thus, Graph ML comes into play. In this blog, I attempt to explain why graph convolutional networks were developed and their underlying mathematics.</p><h2 id=what-are-networksgraphs>What are networks/graphs?<a hidden class=anchor aria-hidden=true href=#what-are-networksgraphs>#</a></h2><p>Networks are a language for expressing large systems comprised of interacting components. Graphs can be used to display a wide variety of data kinds.</p><p>Social Networks, Communication Networks, Network of Neurons, Molecules, and Event Graphs are examples of networks.</p><p>A graph is a two-component data structure in computer science, consisting of nodes (vertices) and edges.
<strong>G = (V, E)</strong>, where <strong>V</strong> is the collection of nodes and <strong>E</strong> are the edges between them, defines a graph.</p><p>Edges are directed if there are directional dependencies between nodes. Otherwise, edges are undirected.</p><p>Graphs are frequently represented by the adjacency matrix <strong>A</strong>. If a graph has <strong>n</strong> nodes, <strong>A</strong> has <strong>(n x n)</strong> dimensions. Occasionally, nodes possess a set of features. If the node has <strong>f</strong> features, then the dimension of the node feature matrix <strong>X</strong> is <strong>(n x f)</strong>.</p><h2 id=why-is-graph-analysis-difficult>Why is graph analysis difficult?<a hidden class=anchor aria-hidden=true href=#why-is-graph-analysis-difficult>#</a></h2><p>The complexity of graph data has presented numerous hurdles for conventional machine learning systems.</p><p>Conventional Machine Learning and Deep Learning tools focus in simple data types. As images with the same size and structure, which can be compared to fixed-size grid graphs. Text and voice are sequences, hence they can be compared to line graphs.</p><p>There are, however, more sophisticated networks without a fixed structure, with variable-sized, unordered nodes that can have varying numbers of neighbours. Existing machine learning methods have the fundamental assumption that instances are independent of one another. This is untrue for graph data, as each node is connected to others by various forms of connections.</p><h2 id=introduction-to-graph-representation-learning>Introduction to graph representation learning<a hidden class=anchor aria-hidden=true href=#introduction-to-graph-representation-learning>#</a></h2><p>Embedding nodes is a basic notion in graph theory. This is accomplished by assigning a weight to each node in the graph and then mapping them to a d-dimensional embedding space (a low-dimensional space rather than the real dimension of the graph) so that nodes with similar properties are embedded close to one another.</p><p><img loading=lazy src=/img/gnn/1.png#center alt=text></p><h3 id=embedding-nodes-in-the-network>Embedding nodes in the network<a hidden class=anchor aria-hidden=true href=#embedding-nodes-in-the-network>#</a></h3><ul><li>Define an encoder</li><li>Define a node similarity function</li><li>Optimize the parameters of the encoder so that:</li></ul><p><strong>Encoder</strong>: Maps a node to a low-dimensional vector</p><p>$$Enc(v) = Zv $$</p><p><strong>Similarity function</strong>: Specifies how the relationships in vector space map to the relationships in the original network</p><p>$$similarity(u,v) \approx Z_u ^ \top Z_v $$</p><h2 id=shallow-encoders-vs-deep-graph-encoders>Shallow encoders vs Deep graph encoders<a hidden class=anchor aria-hidden=true href=#shallow-encoders-vs-deep-graph-encoders>#</a></h2><p>Graphs can be represented in using 2 techniques. Shallow encoders and Deep encoders.</p><h3 id=shallow-encoding>“Shallow” Encoding<a hidden class=anchor aria-hidden=true href=#shallow-encoding>#</a></h3><p>&ldquo;Shallow&rdquo; encoding is the simplest way to encode. It means that the encoder is just an embedding-lookup, and it could be written as:</p><p>$$Enc(v) = Zv $$</p><p>Each column in Z represents an embedding of a node, and the number of rows in Z is equal to the size of the embeddings. v is the indicator vector that points to node v. It has all zeros except for a single one in column v. In &ldquo;shallow&rdquo; encoding, we can see that each node is given a unique embedding vector.</p><p><img loading=lazy src=/img/gnn/3.png#center alt=text></p><h4 id=shallow-encoders-have-their-limits>&ldquo;Shallow Encoders&rdquo; Have Their Limits<a hidden class=anchor aria-hidden=true href=#shallow-encoders-have-their-limits>#</a></h4><p>Since each node has its own embedding, Shallow Encoders don&rsquo;t scale. Shallow Encoders are transductive by nature. It can only make embeddings for a single graph that stays the same. Also the features of nodes are not taken into account.</p><p>Different types of shallow encoders:</p><ul><li>Random walk</li><li>Node2vec</li><li>TransE</li></ul><h3 id=deep-encoding>“Deep” Encoding<a hidden class=anchor aria-hidden=true href=#deep-encoding>#</a></h3><p>The current state of the art performance is achieved using Deep graph
encoders.</p><p>$$Enc(v) = Multiple\space layers\space of\space non\space linear \space transforms $$</p><h2 id=graph-convolutional-networks-gcn>Graph Convolutional Networks (GCN)<a hidden class=anchor aria-hidden=true href=#graph-convolutional-networks-gcn>#</a></h2><p>GCNs are a type of deep graph encoders. The majority of neural networks operate on graphs of fixed size, such as images and texts. However, in the actual world, graph sizes are not fixed. Let&rsquo;s have a look at the computational graphs of GCNs.</p><p>Given a graph <strong>G = (V,A,X)</strong> such that:</p><ul><li>V is the vertex set</li><li>A is the adjacency matrix</li><li>X ∈ R^(m×|V|) is the node feature matrix</li></ul><p><img loading=lazy src=/img/gnn/4.png#center alt=text></p><p>The computational graph must preserve the structure of graph G and incorporate the neighboring characteristics of the nodes. For instance, the embedding vector of node A should consist of characteristics of its neighbors B, C, and D without regard to their order. One technique to accomplish this is by averaging the characteristics of B,C,D.</p><p>With two layers, the computational graph of G will appear as follows:</p><p><img loading=lazy src=/img/gnn/5.png#center alt=text></p><p>The computational graph for node A:</p><p><img loading=lazy src=/img/gnn/6.png#center alt=text></p><h3 id=analogy-between-image-convolution-and-graph-convolution>Analogy between image convolution and graph convolution<a hidden class=anchor aria-hidden=true href=#analogy-between-image-convolution-and-graph-convolution>#</a></h3><p>CNNs and GCNs have a number of operational commonalities.</p><p><img loading=lazy src=/img/gnn/7.png#center alt=text></p><p>In a CNN Conv layer, a window moves across a matrix and aggregates the local representations before proceeding to the next stride. Now, when considering the Graph Conv layer, the representations of nodes are created by aggregating the representations of adjacent nodes. Comparable to the adjacent pixels in a CNN.</p><h3 id=graph-convolutional-layer-and-propagation-rule>Graph convolutional layer and propagation rule<a hidden class=anchor aria-hidden=true href=#graph-convolutional-layer-and-propagation-rule>#</a></h3><p>Every neural network layer can then be written as a nonlinear function</p><p>$$H^{(l+1)} = f(H^{(l)},A) $$</p><h4 id=the-propagation-rule>The propagation rule:<a hidden class=anchor aria-hidden=true href=#the-propagation-rule>#</a></h4><p>$$f(H^{(l)},A) = \sigma (\hat{D}^{-{1 \over 2}}\hat{A}\hat{D}^{-{1 \over 2}}H^{(l)}W^{(l)})$$</p><h4 id=lets-try-to-understand-this-equation-using-code>Let&rsquo;s try to understand this equation using code<a hidden class=anchor aria-hidden=true href=#lets-try-to-understand-this-equation-using-code>#</a></h4><h4 id=1-imports>1. Imports<a hidden class=anchor aria-hidden=true href=#1-imports>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>networkx</span> <span class=k>as</span> <span class=nn>nx</span>
</span></span></code></pre></div><h4 id=2-the-adjacency-matrix>2. The adjacency matrix<a hidden class=anchor aria-hidden=true href=#2-the-adjacency-matrix>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>A</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>matrix</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=n>dtype</span><span class=o>=</span><span class=nb>float</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=n>G</span> <span class=o>=</span> <span class=n>nx</span><span class=o>.</span><span class=n>from_numpy_matrix</span><span class=p>(</span><span class=n>A</span><span class=p>)</span> 
</span></span></code></pre></div><p>This is how the graph looks like</p><p><img loading=lazy src=/img/gnn/8.png#center alt=text></p><h4 id=3-the-feature-matrix>3. The feature matrix<a hidden class=anchor aria-hidden=true href=#3-the-feature-matrix>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>matrix</span><span class=p>([</span><span class=n>a</span><span class=p>,</span><span class=n>b</span><span class=p>,</span><span class=n>a</span><span class=p>,</span><span class=n>b</span><span class=p>])</span> <span class=c1>#features </span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#matrix([[1, 0],</span>
</span></span><span class=line><span class=cl><span class=c1>#        [0, 1],</span>
</span></span><span class=line><span class=cl><span class=c1>#        [1, 0],</span>
</span></span><span class=line><span class=cl><span class=c1>#        [0, 1]])</span>
</span></span></code></pre></div><h4 id=4-propagation-rule>4. Propagation rule<a hidden class=anchor aria-hidden=true href=#4-propagation-rule>#</a></h4><p>Multiplying Adjacency and Feature matrix</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>A</span> <span class=o>@</span> <span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#matrix([[0., 1.],</span>
</span></span><span class=line><span class=cl><span class=c1>#        [1., 1.],</span>
</span></span><span class=line><span class=cl><span class=c1>#        [0., 1.],</span>
</span></span><span class=line><span class=cl><span class=c1>#        [1., 0.]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#  a b</span>
</span></span><span class=line><span class=cl><span class=c1>#a 0 1   a is connected to 0 a and 1 b (Node 0) </span>
</span></span><span class=line><span class=cl><span class=c1>#b 1 1   b is connected to 1 a and 1 b (Node 1)</span>
</span></span><span class=line><span class=cl><span class=c1>#a 0 1   a is connected to 0 a and 1 b (Node 2)</span>
</span></span><span class=line><span class=cl><span class=c1>#b 1 0   b is connected to 1 a and 0 b (Node 3)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#The aggregated representation of a node does not include its own features!</span>
</span></span></code></pre></div><p>Creating an Identity matrix</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>I</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>matrix</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=n>A</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>I</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># matrix([[1., 0., 0., 0.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 1., 0., 0.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 0., 1., 0.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 0., 0., 1.]])</span>
</span></span></code></pre></div><p>Getting the projection matrix</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>A_hat</span> <span class=o>=</span> <span class=n>A</span> <span class=o>+</span> <span class=n>I</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>A_hat</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># matrix([[1., 1., 0., 0.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 1., 1., 1.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 1., 1., 0.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [1., 0., 0., 1.]])</span>
</span></span></code></pre></div><p>Multiplying the projection matrix with feature matrix</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>A_hat</span> <span class=o>@</span> <span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># matrix([[1., 1.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [1., 2.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [1., 1.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [1., 1.]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#  a b</span>
</span></span><span class=line><span class=cl><span class=c1>#a 1 1   a is connected to 1 a and 1 b (Node 0) </span>
</span></span><span class=line><span class=cl><span class=c1>#b 1 2   b is connected to 1 a and 2 b (Node 1)</span>
</span></span><span class=line><span class=cl><span class=c1>#a 1 1   a is connected to 1 a and 1 b (Node 2)</span>
</span></span><span class=line><span class=cl><span class=c1>#b 1 1   b is connected to 1 a and 1 b (Node 3)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#The aggregated representation of a node does include its own features!</span>
</span></span></code></pre></div><p>Yayy now the representation of nodes include themselves!!!</p><p>Now normalizing the representations</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>D_hat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>A_hat</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>))[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>D_hat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>matrix</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>D_hat</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>D_hat</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># matrix([[2., 0., 0., 0.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 3., 0., 0.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 0., 2., 0.],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0., 0., 0., 2.]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>D_hat</span><span class=o>**-</span><span class=mi>1</span> <span class=o>@</span> <span class=n>A_hat</span> <span class=o>@</span> <span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># matrix([[0.5       , 0.5       ],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0.33333333, 0.66666667],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0.5       , 0.5       ],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0.5       , 0.5       ]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Result,</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#matrix([[1., 1.],</span>
</span></span><span class=line><span class=cl>        <span class=c1># [1., 2.],</span>
</span></span><span class=line><span class=cl>        <span class=c1># [1., 1.],</span>
</span></span><span class=line><span class=cl>        <span class=c1># [1., 1.]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># is normalized to</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#matrix([[0.5       , 0.5       ],</span>
</span></span><span class=line><span class=cl>        <span class=c1># [0.33333333, 0.66666667],</span>
</span></span><span class=line><span class=cl>        <span class=c1># [0.5       , 0.5       ],</span>
</span></span><span class=line><span class=cl>        <span class=c1># [0.5       , 0.5       ]])</span>
</span></span></code></pre></div><p>A better way to normalize</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>D_hat_inv</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>D_hat</span><span class=o>**-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>H</span> <span class=o>=</span> <span class=n>D_hat_inv</span> <span class=o>@</span> <span class=n>A_hat</span> <span class=o>@</span> <span class=n>D_hat_inv</span> <span class=o>@</span> <span class=n>X</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>H</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># matrix([[0.5       , 0.40824829],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0.40824829, 0.74158162],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0.5       , 0.40824829],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [0.5       , 0.5       ]])</span>
</span></span></code></pre></div><p>Assigning weights</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#Weights</span>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W</span> <span class=o>=</span> <span class=mi>2</span><span class=o>*</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span><span class=mi>5</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>W</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>matrix</span><span class=p>(</span><span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>W</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># matrix([[ 0.09762701,  0.43037873,  0.20552675,  0.08976637, -0.1526904 ],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [ 0.29178823, -0.12482558,  0.783546  ,  0.92732552, -0.23311696]])</span>
</span></span></code></pre></div><p>Finding the outputs of the hidden layer</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>H</span> <span class=o>@</span> <span class=n>W</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>Z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># matrix([[ 0.16793555,  0.16422954,  0.42264469,  0.42346224, -0.1715148 ],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [ 0.25624085,  0.08313303,  0.66496926,  0.72433453, -0.23521085],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [ 0.16793555,  0.16422954,  0.42264469,  0.42346224, -0.1715148 ],</span>
</span></span><span class=line><span class=cl><span class=c1>#         [ 0.19470762,  0.15277658,  0.49453638,  0.50854594, -0.19290368]])</span>
</span></span></code></pre></div><p>That is the propagation rule!</p><h4 id=4-building-the-graph-conv-layer>4. Building the Graph Conv layer<a hidden class=anchor aria-hidden=true href=#4-building-the-graph-conv-layer>#</a></h4><pre tabindex=0><code>def GraphConv_layer(A_hat, D_hat, X, W):
    D_hat_inv = np.sqrt(D_hat**-1)
    return (D_hat_inv @ A_hat @ D_hat_inv @ X @ W)
</code></pre><p>Which is $$f(H^{(l)},A) = \sigma (\hat{D}^{-{1 \over 2}}\hat{A}\hat{D}^{-{1 \over 2}}H^{(l)}W^{(l)})$$</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ol><li>CS224W: Machine Learning with Graphs: <a href=https://web.stanford.edu/class/cs224w/>https://web.stanford.edu/class/cs224w/</a></li><li>Semi-Supervised Classification with Graph Convolutional Networks: <a href=https://tkipf.github.io/graph-convolutional-networks/>https://tkipf.github.io/graph-convolutional-networks/</a></li><li><a href=https://github.com/PotatoSpudowski/Graph_convolution_basics/blob/master/Embed_molecules_using_GCN.ipynb>Embedding molecules using GCNs</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://potatospudowski.github.io/tags/graph-convolutional-networks/>Graph Convolutional Networks</a></li><li><a href=https://potatospudowski.github.io/tags/graph-ml/>Graph ML</a></li></ul><nav class=paginav><a class=prev href=https://potatospudowski.github.io/posts/attention/><span class=title>« Prev</span><br><span>Understanding Transformers! (Part 1: Model Architecture)</span></a>
<a class=next href=https://potatospudowski.github.io/posts/rul/><span class=title>Next »</span><br><span>RUL(Remaining Useful Life) and SOH(State of Health) estimation of Lithium-ion satellite power systems using Support-Vector-Regression</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Understanding the mathematical foundations and applications of Graph Convolutional Networks on twitter" href="https://twitter.com/intent/tweet/?text=Understanding%20the%20mathematical%20foundations%20and%20applications%20of%20Graph%20Convolutional%20Networks&url=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fgnn%2f&hashtags=GraphConvolutionalNetworks%2cGraphML"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding the mathematical foundations and applications of Graph Convolutional Networks on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fgnn%2f&title=Understanding%20the%20mathematical%20foundations%20and%20applications%20of%20Graph%20Convolutional%20Networks&summary=Understanding%20the%20mathematical%20foundations%20and%20applications%20of%20Graph%20Convolutional%20Networks&source=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fgnn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding the mathematical foundations and applications of Graph Convolutional Networks on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fgnn%2f&title=Understanding%20the%20mathematical%20foundations%20and%20applications%20of%20Graph%20Convolutional%20Networks"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding the mathematical foundations and applications of Graph Convolutional Networks on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fgnn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding the mathematical foundations and applications of Graph Convolutional Networks on whatsapp" href="https://api.whatsapp.com/send?text=Understanding%20the%20mathematical%20foundations%20and%20applications%20of%20Graph%20Convolutional%20Networks%20-%20https%3a%2f%2fpotatospudowski.github.io%2fposts%2fgnn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding the mathematical foundations and applications of Graph Convolutional Networks on telegram" href="https://telegram.me/share/url?text=Understanding%20the%20mathematical%20foundations%20and%20applications%20of%20Graph%20Convolutional%20Networks&url=https%3a%2f%2fpotatospudowski.github.io%2fposts%2fgnn%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://potatospudowski.github.io/>GradientDissent</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>