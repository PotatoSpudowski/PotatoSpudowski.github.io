<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reading - Bahushruth CS</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/svg+xml" href="potato.svg">
</head>
<body>
    <nav class="nav">
        <div class="nav-container">
            <a href="index.html" class="logo" title="Home">
                <img src="potato.svg" alt="Potato" class="potato-icon">
            </a>
            <div class="nav-links">
                <a href="articles.html" class="nav-link">Articles</a>
                <a href="reading.html" class="nav-link active">Reading</a>
                <a href="about.html" class="nav-link">About</a>
            </div>
        </div>
    </nav>

    <main class="main">
        <h1 class="page-title">Reading</h1>
        <p class="page-subtitle">Papers I find interesting on ML, LLMs, and optimization</p>

        <div class="reading-categories">
            <details class="reading-category" open>
                <summary class="category-toggle">
                    <span class="category-title">Transformers & BERT</span>
                    <span class="category-count">4 papers</span>
                </summary>
                <div class="reading-list">
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/1706.03762" class="reading-title" target="_blank">Attention Is All You Need</a>
                            <span class="reading-author">Vaswani et al. (2017)</span>
                        </div>
                        <p class="reading-why">The foundational paper that started the transformer revolution—essential for understanding modern LLMs.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/1810.04805" class="reading-title" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers</a>
                            <span class="reading-author">Devlin et al. (2018)</span>
                        </div>
                        <p class="reading-why">Changed how we think about language representation—bidirectional context was a game changer for NLP.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2007.14062" class="reading-title" target="_blank">Big Bird: Transformers for Longer Sequences</a>
                            <span class="reading-author">Zaheer et al. (2020)</span>
                        </div>
                        <p class="reading-why">Solves the quadratic attention problem—crucial for processing long documents.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2502.12033" class="reading-title" target="_blank">The Geometry of BERT</a>
                            <span class="reading-author">(2025)</span>
                        </div>
                        <p class="reading-why">Fresh perspective on how BERT represents language geometrically.</p>
                    </div>
                </div>
            </details>

            <details class="reading-category">
                <summary class="category-toggle">
                    <span class="category-title">Graph Neural Networks</span>
                    <span class="category-count">5 papers</span>
                </summary>
                <div class="reading-list">
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/1812.08434" class="reading-title" target="_blank">Graph Neural Networks: A Review of Methods and Applications</a>
                            <span class="reading-author">Zhou et al. (2018)</span>
                        </div>
                        <p class="reading-why">Comprehensive survey that ties together GCNs, GATs, and GraphSAGE—essential for relational data.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/1609.02907" class="reading-title" target="_blank">Semi-Supervised Classification with Graph Convolutional Networks</a>
                            <span class="reading-author">Kipf & Welling (2016)</span>
                        </div>
                        <p class="reading-why">The paper that made GNNs practical.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/1710.10903" class="reading-title" target="_blank">Graph Attention Networks</a>
                            <span class="reading-author">Veličković et al. (2017)</span>
                        </div>
                        <p class="reading-why">Attention mechanism adapted for graphs.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/1706.02216" class="reading-title" target="_blank">Inductive Representation Learning on Large Graphs (GraphSAGE)</a>
                            <span class="reading-author">Hamilton et al. (2017)</span>
                        </div>
                        <p class="reading-why">Enables generalization to unseen nodes.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/1810.02244" class="reading-title" target="_blank">Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</a>
                            <span class="reading-author">Morris et al. (2018)</span>
                        </div>
                        <p class="reading-why">Connects GNNs to graph isomorphism testing.</p>
                    </div>
                </div>
            </details>

            <details class="reading-category">
                <summary class="category-toggle">
                    <span class="category-title">LLM Optimization & Efficiency</span>
                    <span class="category-count">5 papers</span>
                </summary>
                <div class="reading-list">
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2106.09685" class="reading-title" target="_blank">LoRA: Low-Rank Adaptation of Large Language Models</a>
                            <span class="reading-author">Hu et al. (2021)</span>
                        </div>
                        <p class="reading-why">Train 10,000x fewer parameters while matching full fine-tuning.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2305.14314" class="reading-title" target="_blank">QLoRA: Efficient Finetuning of Quantized LLMs</a>
                            <span class="reading-author">Dettmers et al. (2023)</span>
                        </div>
                        <p class="reading-why">Fine-tune 65B models on single GPU.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2205.14135" class="reading-title" target="_blank">FlashAttention: Fast and Memory-Efficient Exact Attention</a>
                            <span class="reading-author">Dao et al. (2022)</span>
                        </div>
                        <p class="reading-why">IO-aware algorithm that reduces HBM reads—2-4x speedup.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2407.08608" class="reading-title" target="_blank">FlashAttention-3: Fast and Accurate Attention with Asynchrony</a>
                            <span class="reading-author">Shah et al. (2024)</span>
                        </div>
                        <p class="reading-why">75% utilization on H100 GPUs with FP8.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2602.04998" class="reading-title" target="_blank">Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning</a>
                            <span class="reading-author">Lee et al. (2026)</span>
                        </div>
                        <p class="reading-why">Challenges the LoRA variant hype.</p>
                    </div>
                </div>
            </details>

            <details class="reading-category">
                <summary class="category-toggle">
                    <span class="category-title">RAG & Knowledge Retrieval</span>
                    <span class="category-count">3 papers</span>
                </summary>
                <div class="reading-list">
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2312.10997" class="reading-title" target="_blank">Retrieval-Augmented Generation for Large Language Models: A Survey</a>
                            <span class="reading-author">Gao et al. (2023)</span>
                        </div>
                        <p class="reading-why">Comprehensive overview of RAG architectures.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2501.09136" class="reading-title" target="_blank">Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG</a>
                            <span class="reading-author">Singh et al. (2025)</span>
                        </div>
                        <p class="reading-why">Autonomous agents meet retrieval.</p>
                    </div>
                    <div class="reading-item">
                        <div class="reading-main">
                            <a href="https://arxiv.org/abs/2601.16462" class="reading-title" target="_blank">Graph-Anchored Knowledge Indexing for RAG</a>
                            <span class="reading-author">Liu et al. (2026)</span>
                        </div>
                        <p class="reading-why">Uses graph structures as dynamic knowledge indices.</p>
                    </div>
                </div>
            </details>
        </div>
    </main>

    <footer class="footer">
        <div class="footer-content">
            <p class="footer-text">© 2026 Bahushruth CS</p>
            <div class="footer-social">
                <a href="https://x.com/Bahushruth" target="_blank" class="footer-link">X/Twitter</a>
                <a href="https://www.linkedin.com/in/bahushruth/" target="_blank" class="footer-link">LinkedIn</a>
                <a href="https://github.com/bahushruth" target="_blank" class="footer-link">GitHub</a>
            </div>
        </div>
    </footer>

    <script>
        // Optional: Add smooth animation for details element
        document.querySelectorAll('details').forEach(details => {
            details.addEventListener('toggle', () => {
                if (details.open) {
                    details.style.opacity = '0';
                    setTimeout(() => {
                        details.style.opacity = '1';
                    }, 10);
                }
            });
        });
    </script>
</body>
</html>
