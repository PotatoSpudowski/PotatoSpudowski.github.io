[{"content":"Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.\nDeep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?\nThus, Graph ML comes into play. In this blog, I attempt to explain why graph convolutional networks were developed and their underlying mathematics.\nWhat are networks/graphs? Networks are a language for expressing large systems comprised of interacting components. Graphs can be used to display a wide variety of data kinds.\nSocial Networks, Communication Networks, Network of Neurons, Molecules, and Event Graphs are examples of networks.\nA graph is a two-component data structure in computer science, consisting of nodes (vertices) and edges. G = (V, E), where V is the collection of nodes and E are the edges between them, defines a graph.\nEdges are directed if there are directional dependencies between nodes. Otherwise, edges are undirected.\nGraphs are frequently represented by the adjacency matrix A. If a graph has n nodes, A has (n x n) dimensions. Occasionally, nodes possess a set of features. If the node has f features, then the dimension of the node feature matrix X is (n x f).\nWhy is graph analysis difficult? The complexity of graph data has presented numerous hurdles for conventional machine learning systems.\nConventional Machine Learning and Deep Learning tools focus in simple data types. As images with the same size and structure, which can be compared to fixed-size grid graphs. Text and voice are sequences, hence they can be compared to line graphs.\nThere are, however, more sophisticated networks without a fixed structure, with variable-sized, unordered nodes that can have varying numbers of neighbours. Existing machine learning methods have the fundamental assumption that instances are independent of one another. This is untrue for graph data, as each node is connected to others by various forms of connections.\nIntroduction to graph representation learning Embedding nodes is a basic notion in graph theory. This is accomplished by assigning a weight to each node in the graph and then mapping them to a d-dimensional embedding space (a low-dimensional space rather than the real dimension of the graph) so that nodes with similar properties are embedded close to one another.\nEmbedding nodes in the network Define an encoder Define a node similarity function Optimize the parameters of the encoder so that: Encoder: Maps a node to a low-dimensional vector\n$$Enc(v) = Zv $$\nSimilarity function: Specifies how the relationships in vector space map to the relationships in the original network\n$$similarity(u,v) \\approx Z_u ^ \\top Z_v $$\nShallow encoders vs Deep graph encoders Graphs can be represented in using 2 techniques. Shallow encoders and Deep encoders.\n“Shallow” Encoding \u0026ldquo;Shallow\u0026rdquo; encoding is the simplest way to encode. It means that the encoder is just an embedding-lookup, and it could be written as:\n$$Enc(v) = Zv $$\nEach column in Z represents an embedding of a node, and the number of rows in Z is equal to the size of the embeddings. v is the indicator vector that points to node v. It has all zeros except for a single one in column v. In \u0026ldquo;shallow\u0026rdquo; encoding, we can see that each node is given a unique embedding vector.\n\u0026ldquo;Shallow Encoders\u0026rdquo; Have Their Limits Since each node has its own embedding, Shallow Encoders don\u0026rsquo;t scale. Shallow Encoders are transductive by nature. It can only make embeddings for a single graph that stays the same. Also the features of nodes are not taken into account.\nDifferent types of shallow encoders:\nRandom walk Node2vec TransE “Deep” Encoding The current state of the art performance is achieved using Deep graph encoders.\n$$Enc(v) = Multiple\\space layers\\space of\\space non\\space linear \\space transforms $$\nGraph Convolutional Networks (GCN) GCNs are a type of deep graph encoders. The majority of neural networks operate on graphs of fixed size, such as images and texts. However, in the actual world, graph sizes are not fixed. Let\u0026rsquo;s have a look at the computational graphs of GCNs.\nGiven a graph G = (V,A,X) such that:\nV is the vertex set A is the adjacency matrix X ∈ R^(m×|V|) is the node feature matrix The computational graph must preserve the structure of graph G and incorporate the neighboring characteristics of the nodes. For instance, the embedding vector of node A should consist of characteristics of its neighbors B, C, and D without regard to their order. One technique to accomplish this is by averaging the characteristics of B,C,D.\nWith two layers, the computational graph of G will appear as follows:\nThe computational graph for node A:\nAnalogy between image convolution and graph convolution CNNs and GCNs have a number of operational commonalities.\nIn a CNN Conv layer, a window moves across a matrix and aggregates the local representations before proceeding to the next stride. Now, when considering the Graph Conv layer, the representations of nodes are created by aggregating the representations of adjacent nodes. Comparable to the adjacent pixels in a CNN.\nGraph convolutional layer and propagation rule Every neural network layer can then be written as a nonlinear function\n$$H^{(l+1)} = f(H^{(l)},A) $$\nThe propagation rule: $$f(H^{(l)},A) = \\sigma (\\hat{D}^{-{1 \\over 2}}\\hat{A}\\hat{D}^{-{1 \\over 2}}H^{(l)}W^{(l)})$$\nLet\u0026rsquo;s try to understand this equation using code 1. Imports import numpy as np import networkx as nx 2. The adjacency matrix A = np.matrix([ [0, 1, 0, 0], [0, 0, 1, 1], [0, 1, 0, 0], [1, 0, 0, 0]], dtype=float ) G = nx.from_numpy_matrix(A) This is how the graph looks like\n3. The feature matrix a = [1,0] b = [0,1] X = np.matrix([a,b,a,b]) #features print(X) #matrix([[1, 0], # [0, 1], # [1, 0], # [0, 1]]) 4. Propagation rule Multiplying Adjacency and Feature matrix\nprint(A @ X) #matrix([[0., 1.], # [1., 1.], # [0., 1.], # [1., 0.]]) # a b #a 0 1 a is connected to 0 a and 1 b (Node 0) #b 1 1 b is connected to 1 a and 1 b (Node 1) #a 0 1 a is connected to 0 a and 1 b (Node 2) #b 1 0 b is connected to 1 a and 0 b (Node 3) #The aggregated representation of a node does not include its own features! Creating an Identity matrix\nI = np.matrix(np.eye(A.shape[0])) print(I) # matrix([[1., 0., 0., 0.], # [0., 1., 0., 0.], # [0., 0., 1., 0.], # [0., 0., 0., 1.]]) Getting the projection matrix\nA_hat = A + I print(A_hat) # matrix([[1., 1., 0., 0.], # [0., 1., 1., 1.], # [0., 1., 1., 0.], # [1., 0., 0., 1.]]) Multiplying the projection matrix with feature matrix\nprint(A_hat @ X) # matrix([[1., 1.], # [1., 2.], # [1., 1.], # [1., 1.]]) # a b #a 1 1 a is connected to 1 a and 1 b (Node 0) #b 1 2 b is connected to 1 a and 2 b (Node 1) #a 1 1 a is connected to 1 a and 1 b (Node 2) #b 1 1 b is connected to 1 a and 1 b (Node 3) #The aggregated representation of a node does include its own features! Yayy now the representation of nodes include themselves!!!\nNow normalizing the representations\nD_hat = np.array(np.sum(A_hat, axis=0))[0] D_hat = np.matrix(np.diag(D_hat)) print(D_hat) # matrix([[2., 0., 0., 0.], # [0., 3., 0., 0.], # [0., 0., 2., 0.], # [0., 0., 0., 2.]]) print(D_hat**-1 @ A_hat @ X) # matrix([[0.5 , 0.5 ], # [0.33333333, 0.66666667], # [0.5 , 0.5 ], # [0.5 , 0.5 ]]) # Result, #matrix([[1., 1.], # [1., 2.], # [1., 1.], # [1., 1.]]) # is normalized to #matrix([[0.5 , 0.5 ], # [0.33333333, 0.66666667], # [0.5 , 0.5 ], # [0.5 , 0.5 ]]) A better way to normalize\nD_hat_inv = np.sqrt(D_hat**-1) H = D_hat_inv @ A_hat @ D_hat_inv @ X print(H) # matrix([[0.5 , 0.40824829], # [0.40824829, 0.74158162], # [0.5 , 0.40824829], # [0.5 , 0.5 ]]) Assigning weights\n#Weights np.random.seed(0) W = 2*np.random.rand(2,5) - 1 W = np.matrix(W) print(W) # matrix([[ 0.09762701, 0.43037873, 0.20552675, 0.08976637, -0.1526904 ], # [ 0.29178823, -0.12482558, 0.783546 , 0.92732552, -0.23311696]]) Finding the outputs of the hidden layer\nZ = H @ W print(Z) # matrix([[ 0.16793555, 0.16422954, 0.42264469, 0.42346224, -0.1715148 ], # [ 0.25624085, 0.08313303, 0.66496926, 0.72433453, -0.23521085], # [ 0.16793555, 0.16422954, 0.42264469, 0.42346224, -0.1715148 ], # [ 0.19470762, 0.15277658, 0.49453638, 0.50854594, -0.19290368]]) That is the propagation rule!\n4. Building the Graph Conv layer def GraphConv_layer(A_hat, D_hat, X, W): D_hat_inv = np.sqrt(D_hat**-1) return (D_hat_inv @ A_hat @ D_hat_inv @ X @ W) Which is $$f(H^{(l)},A) = \\sigma (\\hat{D}^{-{1 \\over 2}}\\hat{A}\\hat{D}^{-{1 \\over 2}}H^{(l)}W^{(l)})$$\nReferences CS224W: Machine Learning with Graphs: https://web.stanford.edu/class/cs224w/ Semi-Supervised Classification with Graph Convolutional Networks: https://tkipf.github.io/graph-convolutional-networks/ Embedding molecules using GCNs ","permalink":"https://potatospudowski.github.io/posts/gnn/","summary":"Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.\nDeep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?","title":"Understanding the mathematical foundations and applications of Graph Convolutional Networks"},{"content":" Batteries are a critical part of any satellite\u0026rsquo;s power system.\nThey are used to provide power:\nDuring launch and after the launch of the satellite till the solar panels are deployed To the spacecraft, its equipment, and payload during the shadow phase For communication and data transfer To maintain the electronics at a specific temperature Batteries with higher gravimetric(higher mass) and volumetric(higher volume) energy densities lead to lesser mass and volume for the power systems and thereby increase payload and mission capabilities.\nA Saft space battery pack comprised of VES180SA space cells. Credit: SpaceNews That is why Lithium-ion batteries are so extensively used in satellites as power systems. A NiCD(nickel-cadmium) battery pack has as much as 50% less energy density as Li-ion batteries.\nBatteries tend to degrade after continuous charge and discharge cycles. There is a need to develop an accurate remaining useful life(RUL) estimation system for Li-ion batteries. The estimated RUL can provide useful information to the \u0026lsquo;health management and maintenance\u0026rsquo; and the \u0026lsquo;ground reliability assessment\u0026rsquo; team. From this information, the maintenance team can plan the maintenance task schedule.\nThe approach of Lithium-ion battery RUL estimation Model-based approach Data-driven approach The model-based approach makes use of battery characteristics and physical structure. The data-driven approach is not based on accurately modeling the physics of a system like the model-based approach but instead estimates RUL based on historical data.\nI will be using the data-driven approach for this project.\nThe Dataset The dataset used for this project can be downloaded here. This data has been collected from prognostics testbed at the NASA Ames Prognostics Center of Excellence (PCoE). The experiments were conducted until a 30% drop in the battery capacity(from 2 A·h to 1.4 A·h) was observed. This time-series data can be used for the development of prognostic algorithms that can accurately predict the RUL of Li-ion batteries.\nData Structure: cycle:\ttop level structure array containing the charge, discharge and impedance operations type: operation type, can be charge, discharge or impedance ambient_temperature:\tambient temperature (degree C) time: the date and time of the start of the cycle, in MATLAB date vector format data:\tdata structure containing the measurements for charge the fields are: Voltage_measured: Battery terminal voltage (Volts) Current_measured:\tBattery output current (Amps) Temperature_measured: Battery temperature (degree C) Current_charge:\tCurrent measured at charger (Amps) Voltage_charge:\tVoltage measured at charger (Volts) Time:\tTime vector for the cycle (secs) for discharge the fields are: Voltage_measured: Battery terminal voltage (Volts) Current_measured:\tBattery output current (Amps) Temperature_measured: Battery temperature (degree C) Current_charge:\tCurrent measured at load (Amps) Voltage_charge:\tVoltage measured at load (Volts) Time:\tTime vector for the cycle (secs) Capacity:\tBattery capacity (Ahr) for discharge till 2.7V for impedance the fields are: Sense_current:\tCurrent in sense branch (Amps) Battery_current:\tCurrent in battery branch (Amps) Current_ratio:\tRatio of the above currents Battery_impedance:\tBattery impedance (Ohms) computed from raw data Rectified_impedance:\tCalibrated and smoothed battery impedance (Ohms) Re:\tEstimated electrolyte resistance (Ohms) Rct:\tEstimated charge transfer resistance (Ohms) Exploratory data analysis The battery data was provided in .mat format, I wrote a python script that parses .mat files and converts them into JSON objects. def create_correspondence_data(self): dom = PyQuery(self.url) writer = csv.writer(open(\u0026#39;correspondence.csv\u0026#39;, \u0026#39;wb\u0026#39;)) for i, img in enumerate(dom(\u0026#39;img\u0026#39;).items()): img_src = img.attr[\u0026#39;src\u0026#39;] print(\u0026#34;%d =\u0026gt; %s\u0026#34; % (i + 1, img_src)) writer.writerow([i + 1, img_src]) For visualization, I used the Matplotlib library.\nLet us take a look at the battery capacity degradation at different ambient temperatures.\nFrom the above diagrams, it can be noted that the capacity is not always continuously decreasing with every discharge cycle. Sometimes the capacity increases(eg cycle no 90 for the ambient temperature 24° C) resulting in distinctive spikes.\nThis is because of the self-charging effect of lithium-ion batteries. The explanation for this effect is during battery use chemical reactions take place, and the electrodes are deposited with chemical products, resulting in reduced chemical reactions. In order to melt the chemical products deposited, The battery requires a short rest period. The capacity of the battery increases suddenly owing to an increase in the available capacity in the next cycle.\nComparison of capacity degradation at different ambient temperature Plotting the capacity degradation data of the different ambient temperatures on a single plot we can see that the optimum temperature for the best performance of the battery is when the ambient temperature is 24°C since the capacity is higher than the battery maintained at 4°C and the degradation rate is lesser compared to the battery maintained at 43°C.\nCharging performance Let us now visualize the data and take a look at the charging profile of batteries at different temperatures.\nThe different columns of plots are:\nVoltage vs Time Current vs Time Temperature(Battery not Ambient temperature) vs Time Voltage vs Time The voltage increases non linearly until a point and then becomes constant. This is because the rated voltage of the battery is 4.2 so the voltage of the battery becomes constant at that point. This phase where the voltage increases non linearly is called the constant current phase here constant current is supplied to the battery till it becomes 4.2 volts. The phase where the voltage is constant is called the constant voltage phase since the voltage maintained here is constant.\nCurrent vs Time As mentioned previously, the constant current phase is a place where a constant current is supplied to the battery. During the constant voltage phase, the current declines non linearly in order to maintain a constant voltage. This is known as Trickle charging, where the fully charged battery is charged under load at a rate equal to its self-discharge rate, thus enabling the battery to remain at its fully charged level.\nTemperature vs Time Temperature is the highest at the point where the phase transitions from the constant current phase to the constant voltage phase.\nFindings It can be seen that the batteries maintained at a lower temperature seem to reach the maximum voltage a lot sooner. As the cycle number increases the battery charging time increases with the exception of the battery maintained at 43°C. Further analysis of this is needed. After looking at the graphs it can be seen that this is indeed true. The higher the clock number, the lesser is the charging time for the battery maintained at an ambient temperature of 43°C.\nDischarge performance Similarly, let us look at the discharge performance of batteries at different temperatures.\nThe different columns of plots are:\nVoltage vs Time Current vs Time Temperature(Battery not Ambient temperature) vs Time Voltage vs Time During a discharge cycle, the voltage discharges non linearly until the voltage becomes 0. After the voltage becomes 0, the voltage increases suddenly for a brief moment. This is due to the self-charging effect which was discussed earlier that increases the capacity of the battery for the next cycle. Current vs Time During a discharge cycle, a constant current is provided by the battery until the battery voltage becomes 0.\nTemperature vs Time Temperature is the highest when the battery voltage just reaches 0. After this phase, Self-charging phenomenon takes place due to the melting of deposits on the electrodes in the absence of chemical reactions. Findings It can be seen that the batteries maintained at an ambient temperature of 24°C last a lot longer during the discharge cycle compared to the other two ambient temperature. As the cycle number increases the battery charging time decreases with the exception of the battery maintained at 4°C. Further analysis of this is needed. After looking at the graphs it can be seen that this isn\u0026rsquo;t the case for all the cycles. The higher the clock number, the lesser is the discharging time, This makes sense as the capacity of the battery is known to decrease along with usage not increase. The capacity at cycle 0 was low because it was a new battery.\nModel building Now let us build a model to fit the data but first, we need to identify and remove outliers to increase prediction performance.\nThe outliers were detected using the rolling standard deviation method. Below is the python implementation of the rolling standard deviation method to identify outliers in the data.\ndef moving_average(data, window_size): \u0026#34;\u0026#34;\u0026#34; Computes moving average using discrete linear convolution of two one dimensional sequences. Args: ----- data (pandas.Series): independent variable window_size (int): rolling window size Returns: -------- ndarray of linear convolution API Reference: https://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html\u0026#34;\u0026#34;\u0026#34; window = np.ones(int(window_size))/float(window_size) return np.convolve(data, window, \u0026#39;same\u0026#39;) def rollingAverage(x_stuff, y_stuff): \u0026#34;\u0026#34;\u0026#34; Helps in exploring the anamolies using rolling standard deviation Args: ----- x_stuff (pandas.Series): X variable y_stuff (pandas.Series): Y variable Returns: -------- lst_x: List of X values where an outier was identified lst_y: List of y values where an outier was identified \u0026#34;\u0026#34;\u0026#34; window_size = 10 #rolling window size sigma=1.0 #value for standard deviation avg = moving_average(y_stuff, window_size) avg_list = avg.tolist() residual = y_stuff - avg # Calculate the variation in the distribution of the residual testing_std = residual.rolling(window_size).std() testing_std_as_df = pd.DataFrame(testing_std) rolling_std = testing_std_as_df.replace(np.nan, testing_std_as_df.iloc[window_size - 1]).round(3).iloc[:,0].tolist() rolling_std std = np.std(residual) lst=[] lst_index = 0 lst_count = 0 for i in y_stuff.index: if (y_stuff[i] \u0026gt; avg_list[lst_index] + (1.5 * rolling_std[lst_index])) | (y_stuff[i] \u0026lt; avg_list[lst_index] - (1.5 * rolling_std[lst_index])): lt=[i,x_stuff[i], y_stuff[i],avg_list[lst_index],rolling_std[lst_index]] lst.append(lt) lst_count+=1 lst_index+=1 lst_x = [] lst_y = [] for i in range (0,len(lst)): lst_x.append(lst[i][1]) lst_y.append(lst[i][2]) return lst_x, lst_y The plot of capacity degradation along with identified outliers.\nIdentifying whether to use a linear model vs a non-linear model for RUL estimation\nResidual plots can be useful to identify if a linear model is to be used or a non-linear.\nThe plot pattern that we obtain from the data is not random(U-shaped). This pattern indicates that a non-linear model is a better choice than a linear model to fit this data. For more information regarding residual plots visit this link.\nSupport vector regression Support vector machines can not only be used for the classification problem but also for regression. The kernel used is RBF(radial basis function).\nPython implementation of the code can be found below.\n# Import the library from sklearn.svm import SVR # Create a support vector regression model svr = SVR(C=20, epsilon=0.0001, gamma=0.00001, cache_size=200, kernel=\u0026#39;rbf\u0026#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False) svr.fit(X_train,y_train) # Fit the model y_pred = svr.predict(X_test) # Perform prediction Once the model is fit, The model can be used to estimate the capacity by taking cycle no as the input. The values of hyperparameters like C, epsilon, and gamma to obtain the best performance can be found out by using grid search and other parameter optimization techniques.\nNow let us compare the model by fitting it to different batteries with different train size.\nBattery 05 Battery 06 Battery 07 Battery 18 Conclusion Support vector regression can be used to build an accurate and effective RUL estimation system for Li-ion batteries, given that the hyperparameters of the SVR are properly chosen.\nAll the code and Jupyter notebook implementation of this project can be found on my GitHub here.\n","permalink":"https://potatospudowski.github.io/posts/rul/","summary":"Batteries are a critical part of any satellite\u0026rsquo;s power system.\nThey are used to provide power:\nDuring launch and after the launch of the satellite till the solar panels are deployed To the spacecraft, its equipment, and payload during the shadow phase For communication and data transfer To maintain the electronics at a specific temperature Batteries with higher gravimetric(higher mass) and volumetric(higher volume) energy densities lead to lesser mass and volume for the power systems and thereby increase payload and mission capabilities.","title":"RUL(Remaining Useful Life) and SOH(State of Health) estimation of Lithium-ion satellite power systems using Support-Vector-Regression"}]