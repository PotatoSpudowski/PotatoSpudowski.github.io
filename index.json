[{"content":"In the past, we\u0026rsquo;ve had different architectures for different modalities of data like CNNs for Images, RNNs for Text and GNNs for Graphs. Recently we have seen the adoption of Transformers for processing all types of data modalities. Transformers can be thought of like general purpose trainable architecture. Since the adoption of transformers has been growing so rapidly, It might be a good idea to revisit the original paper.\nIntroduction In sequence modelling and transduction problems, recurrent neural networks, and in particular extended short-term memory and gated recurrent neural networks, have become standard. Computational recurrence is often decomposed in recurrent models according to the symbol positions in the input and output sequences. Due to its intrinsic sequential nature, training examples cannot be parallelized. Recent work has improved computational efficiency significantly by employing factorization methods. However, the underlying barrier of sequential processing persists.\nAttention mechanisms have evolved to the point where they are now an indispensable component of compelling sequence modelling and transduction models in a variety of tasks. This has made it possible to model dependencies without taking into account the order in which they appear in the input or output sequences. However, in virtually every instance, such attention mechanisms are used in conjunction with a recurrent neural network\nIn order to create global relationships between input and output, the paper\u0026rsquo;s authors propose the Transformer, a model architecture that does not use recursion but rather relies solely on an attention mechanism. The Transformer can achieve a new state-of-the-art in translation quality and allows for substantially higher parallelization.\nBackground Reducing Sequential Computation in Deep Neural Networks The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S. All of these use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions. This makes it more difficult to learn dependencies between distant positions.\nSelf-Attention: An Intra-Attention Approach Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.\nHas self-attention been used successfully in a variety of tasks? A recurrent attention mechanism, as opposed to a sequence-aligned recurrence mechanism, is used as the foundation for end-to-end memory networks. It has been demonstrated that they are successful at answering questions in straightforward English and in language modelling activities.\nTransformer is the first transduction model that rely solely on self-attention to compute representations of its input and output.\nModel Architecture Auto-Regressive Neural Sequence Transduction Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps a sequence of symbol representations (x1,\u0026hellip;,xn) to a sequence of continuous representations (z = z1,\u0026hellip;,zn). When given z, the decoder then creates a sequence of symbols (y1,\u0026hellip;, ym) one at a time. At each step, the model is auto-regressive, which means that it uses the symbols it has already made as input for the next step.\nFor both the encoder and the decoder, the Transformer has stacked self-attention and point-by-point layers that are all fully connected.\nclass EncoderDecoder(nn.Module): \u0026#34;\u0026#34;\u0026#34; A standard Encoder-Decoder architecture. Base for this and many other models. \u0026#34;\u0026#34;\u0026#34; def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder, self).__init__() self.encoder = encoder self.decoder = decoder self.src_embed = src_embed self.tgt_embed = tgt_embed self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): \u0026#34;Take in and process masked src and target sequences.\u0026#34; return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) class Generator(nn.Module): \u0026#34;Define standard linear + softmax generation step.\u0026#34; def __init__(self, d_model, vocab): super(Generator, self).__init__() self.proj = nn.Linear(d_model, vocab) def forward(self, x): return log_softmax(self.proj(x), dim=-1) Encoder and Decoder Stacks Encoder There are N = 6 layers in the encoder, each of which is identical in every way possible.\nThe layers are divided into two sublayers each.\nMulti-head self-attention mechanism, Simple, position-wise fully connected feed-forward network. We first normalise the layer and then use a residual connection around each of the two sublayers. LayerNorm(x + Sublayer(x)) is the output of a sub-layer, where Sublayer(x) is the function implemented by that sub-layer. All model sublayers and embedding layers generate 512-dimensional outputs to facilitate these residual connections.\ndef clones(module, N): return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) class Encoder(nn.Module): def __init__(self, layer, N): super(Encoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, mask): for layer in self.layers: x = layer(x, mask) return self.norm(x) class LayerNorm(nn.Module): \u0026#34;\u0026#34;\u0026#34; Layer Norm module LayerNorm is a type of normalization that is applied to the output of each sub-layer in the encoder. This normalization helps to improve the stability of the AI model and makes it easier for the model to learn. Layer normalisation tries to diminish the impact of covariant shift. In other words, it prevents the mean and standard deviation of embedding vector elements from shifting, which renders training unstable and sluggish. \u0026#34;\u0026#34;\u0026#34; def __init__(self, features, eps=1e-6): super(LayerNorm, self).__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2 = nn.Parameter(torch.zeros(features)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim=True) std = x.std(-1, keepdim=True) return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 class EncoderLayer(nn.Module): def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 2) self.size = size def forward(self, x, mask): x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) return self.sublayer[1](x, self.feed_forward) Decoder In the same way that the encoder is made up of N = 6 levels, the decoder is also built up of identical layers. In order to execute multi-head attention on the encoder stack\u0026rsquo;s output, the decoder adds a third sub-layer on top of the original two in each encoder layer. We use residual connections around each of the sub-layers, much like the encoder does, and then we normalise the layers. Moreover, we tweak the decoder stack\u0026rsquo;s self-attention sub-layer to make positions uninterested in focusing on those that come after them. This masking, in conjunction with the output embeddings\u0026rsquo; positional offset of one, ensures that the predictions for position i can only depend on the known outputs at positions less than i.\nWhen we talk about masked multi-head attention, this indicates that the multi-head attention receives inputs that are disguised such that the attention mechanism does not use any of the information from the positions that are hidden. The researchers applied the mask within the attention computation, according to the study, and they mention that they did so by setting attention scores to a value greater than negative infinity (or a very large negative number). Masked places are given an effective probability of zero thanks to the softmax function found within the attention systems.\n(1, 0, 0, 0, 0, …, 0) =\u0026gt; (\u0026lt;SOS\u0026gt;) (1, 1, 0, 0, 0, …, 0) =\u0026gt; (\u0026lt;SOS\u0026gt;, ‘Friday’) (1, 1, 1, 0, 0, …, 0) =\u0026gt; (\u0026lt;SOS\u0026gt;, ‘Friday’, ‘hai’) (1, 1, 1, 1, 0, …, 0) =\u0026gt; (\u0026lt;SOS\u0026gt;, ‘Friday’, ‘hai’, \u0026#39;pencho\u0026#39;) (1, 1, 1, 1, 1, …, 0) =\u0026gt; (\u0026lt;SOS\u0026gt;, \u0026#39;Friday\u0026#39;, \u0026#39;hai\u0026#39;, \u0026#39;pencho\u0026#39;, \u0026#39;!\u0026#39;) Another type of multi-head attention, the source-target attention, determines the attention values between the features (embeddings) of the input sentence and the features of the output (still partial) sentence. It does this by calculating the distance between the two sets of features.\nclass Decoder(nn.Module): def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x) class DecoderLayer(nn.Module): def __init__(self, size, self_attn, src_attn, feed_forward, dropout): super(DecoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.src_attn = src_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 3) def forward(self, x, memory, src_mask, tgt_mask): m = memory x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) return self.sublayer[2](x, self.feed_forward) Position-wise Feed-Forward Networks Encoder and decoder layers both have attention sub-layers and a fully connected feed-forward network, which are applied independently and uniformly to each position. This is made up of two linear transformations separated by a ReLU activation.\n$$FFN(x) = max(0, xW_1+b_1)W_2 + b_2$$\nDespite the fact that the linear transformations are identical across all places, their parameters vary from layer to layer. One other approach to explain this would be to say that it consists of two convolutions with a kernel size of 1. The dimensionality of input and output is\n$$d_{model} = 512$$\nand the inner-layer has dimensionality\n$$d_{ff} = 2048$$\nclass PositionwiseFeedForward(nn.Module): \u0026#34;\u0026#34;\u0026#34; Multiplying x by W1 doubles its size to 2048, then dividing it by W2 brings it back down to 512. In FFN, the weights for all positions inside the same layer are the same. \u0026#34;\u0026#34;\u0026#34; def __init__(self, d_model, d_ff, dropout=0.1): super(PositionwiseFeedForward, self).__init__() self.w_1 = nn.Linear(d_model, d_ff) self.w_2 = nn.Linear(d_ff, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): return self.w_2(self.dropout(self.w_1(x).relu())) Embeddings and Softmax In a manner analogous to that of previous sequence transduction models, authors make use of learnt embeddings in order to transform input tokens and output tokens into vectors with the dimension.\n$$d_{model}$$\nThey convert the decoder output into projected next-token probabilities by employing the standard learnt linear transformation in conjunction with the softmax function. Within the model, they make use of the same weight matrix for both of the embedding layers as well as the pre-softmax linear transformation. Within the embedding layers, they multiply those weights by the square root of the model\u0026rsquo;s dimension.\n$$\\sqrt{d_{model}}$$ ​\nclass Embeddings(nn.Module): def __init__(self, d_model, vocab): super(Embeddings, self).__init__() self.lut = nn.Embedding(vocab, d_model) self.d_model = d_model def forward(self, x): return self.lut(x) * math.sqrt(self.d_model) Positional Encoding I\u0026rsquo;ve saved my favorite for last. When and why do we require positional encoding?\nA non-recurrent architecture of multi-head attention, Transformer employs positional encoding to provide the order context. When recurrent neural networks are given sequence inputs, the input itself defines the sequential order (ordering of time-steps). But the Transformer\u0026rsquo;s Multi-Head Attention layer is a feed-forward layer, which takes in the entire sequence at once instead of sequentially step by step.. Attention is sequence-independent since it is computed on each datapoint (time-step) independently, hence it does not take into account the ordering of datapoints.\nThe principle of positional encoding is employed to solve this issue. Simply put, this entails adding a tensor to the input sequence that has the desired characteristics. To do this, \u0026ldquo;positional encodings\u0026rdquo; are added to the input embeddings at the base of the encoder and decoder stacks. Positional encodings and embeddings are of the same dimension, allowing for a simple addition of the two.\nThey employ sine and cosine functions with varying frequencies:\n$$ PE_{(pos, 2i)} = sin(pos/1000^{2i/d_{model}}) $$ $$ PE_{(pos, 2{i+1})} = cos(pos/1000^{2i/d_{model}}) $$\nwhere pos is the location and i represents the size. That is, a sinusoid represents one dimension of the positional encoding. The wavelengths increase geometrically from 2 to 100002. Authors opted for this function on the assumption that it would facilitate the model\u0026rsquo;s ability to pick up on relative positional cues, since for every fixed offset k,\n$$ PE_{pos+k} = \\textrm{linear \\ function \\ of } PE_{pos+k} $$\nThe total embedding and positional encoding sums in the encoder and decoder stacks are also subjected to dropout. The starting point for the base model is a rate of\n$$P_{drop} = 0.1$$ ​\nclass PositionalEncoding(nn.Module): def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp( torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model) ) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer(\u0026#34;pe\u0026#34;, pe) def forward(self, x): x = x + self.pe[:, : x.size(1)].requires_grad_(False) return self.dropout(x) Thats all for now 🤭\nIn the next part, We will try to understand the attention mechanism in transformers! I felt like it would be better to have a separate post instead of writing about it here.\nReferences Attention Is All You Need https://arxiv.org/abs/1706.03762 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nThe Annotated Transformer http://nlp.seas.harvard.edu/2018/04/03/attention.html Harvard NLP\nThe Illustrated Transformer http://jalammar.github.io/illustrated-transformer/ Jay Alammar\nTransformer break-down: Positional Encoding https://medium.datadriveninvestor.com/transformer-break-down-positional-encoding-c8d1bbbf79a8\n","permalink":"https://potatospudowski.github.io/posts/attention/","summary":"In the past, we\u0026rsquo;ve had different architectures for different modalities of data like CNNs for Images, RNNs for Text and GNNs for Graphs. Recently we have seen the adoption of Transformers for processing all types of data modalities. Transformers can be thought of like general purpose trainable architecture. Since the adoption of transformers has been growing so rapidly, It might be a good idea to revisit the original paper.\nIntroduction In sequence modelling and transduction problems, recurrent neural networks, and in particular extended short-term memory and gated recurrent neural networks, have become standard.","title":"Understanding Transformers! (Part 1: Model Architecture)"},{"content":"Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.\nDeep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?\nThus, Graph ML comes into play. In this blog, I attempt to explain why graph convolutional networks were developed and their underlying mathematics.\nWhat are networks/graphs? Networks are a language for expressing large systems comprised of interacting components. Graphs can be used to display a wide variety of data kinds.\nSocial Networks, Communication Networks, Network of Neurons, Molecules, and Event Graphs are examples of networks.\nA graph is a two-component data structure in computer science, consisting of nodes (vertices) and edges. G = (V, E), where V is the collection of nodes and E are the edges between them, defines a graph.\nEdges are directed if there are directional dependencies between nodes. Otherwise, edges are undirected.\nGraphs are frequently represented by the adjacency matrix A. If a graph has n nodes, A has (n x n) dimensions. Occasionally, nodes possess a set of features. If the node has f features, then the dimension of the node feature matrix X is (n x f).\nWhy is graph analysis difficult? The complexity of graph data has presented numerous hurdles for conventional machine learning systems.\nConventional Machine Learning and Deep Learning tools focus in simple data types. As images with the same size and structure, which can be compared to fixed-size grid graphs. Text and voice are sequences, hence they can be compared to line graphs.\nThere are, however, more sophisticated networks without a fixed structure, with variable-sized, unordered nodes that can have varying numbers of neighbours. Existing machine learning methods have the fundamental assumption that instances are independent of one another. This is untrue for graph data, as each node is connected to others by various forms of connections.\nIntroduction to graph representation learning Embedding nodes is a basic notion in graph theory. This is accomplished by assigning a weight to each node in the graph and then mapping them to a d-dimensional embedding space (a low-dimensional space rather than the real dimension of the graph) so that nodes with similar properties are embedded close to one another.\nEmbedding nodes in the network Define an encoder Define a node similarity function Optimize the parameters of the encoder so that: Encoder: Maps a node to a low-dimensional vector\n$$Enc(v) = Zv $$\nSimilarity function: Specifies how the relationships in vector space map to the relationships in the original network\n$$similarity(u,v) \\approx Z_u ^ \\top Z_v $$\nShallow encoders vs Deep graph encoders Graphs can be represented in using 2 techniques. Shallow encoders and Deep encoders.\n“Shallow” Encoding \u0026ldquo;Shallow\u0026rdquo; encoding is the simplest way to encode. It means that the encoder is just an embedding-lookup, and it could be written as:\n$$Enc(v) = Zv $$\nEach column in Z represents an embedding of a node, and the number of rows in Z is equal to the size of the embeddings. v is the indicator vector that points to node v. It has all zeros except for a single one in column v. In \u0026ldquo;shallow\u0026rdquo; encoding, we can see that each node is given a unique embedding vector.\n\u0026ldquo;Shallow Encoders\u0026rdquo; Have Their Limits Since each node has its own embedding, Shallow Encoders don\u0026rsquo;t scale. Shallow Encoders are transductive by nature. It can only make embeddings for a single graph that stays the same. Also the features of nodes are not taken into account.\nDifferent types of shallow encoders:\nRandom walk Node2vec TransE “Deep” Encoding The current state of the art performance is achieved using Deep graph encoders.\n$$Enc(v) = Multiple\\space layers\\space of\\space non\\space linear \\space transforms $$\nGraph Convolutional Networks (GCN) GCNs are a type of deep graph encoders. The majority of neural networks operate on graphs of fixed size, such as images and texts. However, in the actual world, graph sizes are not fixed. Let\u0026rsquo;s have a look at the computational graphs of GCNs.\nGiven a graph G = (V,A,X) such that:\nV is the vertex set A is the adjacency matrix X ∈ R^(m×|V|) is the node feature matrix The computational graph must preserve the structure of graph G and incorporate the neighboring characteristics of the nodes. For instance, the embedding vector of node A should consist of characteristics of its neighbors B, C, and D without regard to their order. One technique to accomplish this is by averaging the characteristics of B,C,D.\nWith two layers, the computational graph of G will appear as follows:\nThe computational graph for node A:\nAnalogy between image convolution and graph convolution CNNs and GCNs have a number of operational commonalities.\nIn a CNN Conv layer, a window moves across a matrix and aggregates the local representations before proceeding to the next stride. Now, when considering the Graph Conv layer, the representations of nodes are created by aggregating the representations of adjacent nodes. Comparable to the adjacent pixels in a CNN.\nGraph convolutional layer and propagation rule Every neural network layer can then be written as a nonlinear function\n$$H^{(l+1)} = f(H^{(l)},A) $$\nThe propagation rule: $$f(H^{(l)},A) = \\sigma (\\hat{D}^{-{1 \\over 2}}\\hat{A}\\hat{D}^{-{1 \\over 2}}H^{(l)}W^{(l)})$$\nLet\u0026rsquo;s try to understand this equation using code 1. Imports import numpy as np import networkx as nx 2. The adjacency matrix A = np.matrix([ [0, 1, 0, 0], [0, 0, 1, 1], [0, 1, 0, 0], [1, 0, 0, 0]], dtype=float ) G = nx.from_numpy_matrix(A) This is how the graph looks like\n3. The feature matrix a = [1,0] b = [0,1] X = np.matrix([a,b,a,b]) #features print(X) #matrix([[1, 0], # [0, 1], # [1, 0], # [0, 1]]) 4. Propagation rule Multiplying Adjacency and Feature matrix\nprint(A @ X) #matrix([[0., 1.], # [1., 1.], # [0., 1.], # [1., 0.]]) # a b #a 0 1 a is connected to 0 a and 1 b (Node 0) #b 1 1 b is connected to 1 a and 1 b (Node 1) #a 0 1 a is connected to 0 a and 1 b (Node 2) #b 1 0 b is connected to 1 a and 0 b (Node 3) #The aggregated representation of a node does not include its own features! Creating an Identity matrix\nI = np.matrix(np.eye(A.shape[0])) print(I) # matrix([[1., 0., 0., 0.], # [0., 1., 0., 0.], # [0., 0., 1., 0.], # [0., 0., 0., 1.]]) Getting the projection matrix\nA_hat = A + I print(A_hat) # matrix([[1., 1., 0., 0.], # [0., 1., 1., 1.], # [0., 1., 1., 0.], # [1., 0., 0., 1.]]) Multiplying the projection matrix with feature matrix\nprint(A_hat @ X) # matrix([[1., 1.], # [1., 2.], # [1., 1.], # [1., 1.]]) # a b #a 1 1 a is connected to 1 a and 1 b (Node 0) #b 1 2 b is connected to 1 a and 2 b (Node 1) #a 1 1 a is connected to 1 a and 1 b (Node 2) #b 1 1 b is connected to 1 a and 1 b (Node 3) #The aggregated representation of a node does include its own features! Yayy now the representation of nodes include themselves!!!\nNow normalizing the representations\nD_hat = np.array(np.sum(A_hat, axis=0))[0] D_hat = np.matrix(np.diag(D_hat)) print(D_hat) # matrix([[2., 0., 0., 0.], # [0., 3., 0., 0.], # [0., 0., 2., 0.], # [0., 0., 0., 2.]]) print(D_hat**-1 @ A_hat @ X) # matrix([[0.5 , 0.5 ], # [0.33333333, 0.66666667], # [0.5 , 0.5 ], # [0.5 , 0.5 ]]) # Result, #matrix([[1., 1.], # [1., 2.], # [1., 1.], # [1., 1.]]) # is normalized to #matrix([[0.5 , 0.5 ], # [0.33333333, 0.66666667], # [0.5 , 0.5 ], # [0.5 , 0.5 ]]) A better way to normalize\nD_hat_inv = np.sqrt(D_hat**-1) H = D_hat_inv @ A_hat @ D_hat_inv @ X print(H) # matrix([[0.5 , 0.40824829], # [0.40824829, 0.74158162], # [0.5 , 0.40824829], # [0.5 , 0.5 ]]) Assigning weights\n#Weights np.random.seed(0) W = 2*np.random.rand(2,5) - 1 W = np.matrix(W) print(W) # matrix([[ 0.09762701, 0.43037873, 0.20552675, 0.08976637, -0.1526904 ], # [ 0.29178823, -0.12482558, 0.783546 , 0.92732552, -0.23311696]]) Finding the outputs of the hidden layer\nZ = H @ W print(Z) # matrix([[ 0.16793555, 0.16422954, 0.42264469, 0.42346224, -0.1715148 ], # [ 0.25624085, 0.08313303, 0.66496926, 0.72433453, -0.23521085], # [ 0.16793555, 0.16422954, 0.42264469, 0.42346224, -0.1715148 ], # [ 0.19470762, 0.15277658, 0.49453638, 0.50854594, -0.19290368]]) That is the propagation rule!\n4. Building the Graph Conv layer def GraphConv_layer(A_hat, D_hat, X, W): D_hat_inv = np.sqrt(D_hat**-1) return (D_hat_inv @ A_hat @ D_hat_inv @ X @ W) Which is $$f(H^{(l)},A) = \\sigma (\\hat{D}^{-{1 \\over 2}}\\hat{A}\\hat{D}^{-{1 \\over 2}}H^{(l)}W^{(l)})$$\nReferences CS224W: Machine Learning with Graphs: https://web.stanford.edu/class/cs224w/ Semi-Supervised Classification with Graph Convolutional Networks: https://tkipf.github.io/graph-convolutional-networks/ Embedding molecules using GCNs ","permalink":"https://potatospudowski.github.io/posts/gnn/","summary":"Recent advances in neural networks have driven study of data mining and pattern recognition. End-to-end deep learning models including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and autoencoders have breathed new life into traditional machine learning tasks such as object detection, machine translation, and speech recognition.\nDeep Learning is effective at uncovering hidden patterns among Euclidean data (images, text, videos). But what about applications that rely on data originating from non-Euclidean domains, which is typically represented as a graph with intricate interdependencies and relationships among objects?","title":"Understanding the mathematical foundations and applications of Graph Convolutional Networks"},{"content":" Batteries are a critical part of any satellite\u0026rsquo;s power system.\nThey are used to provide power:\nDuring launch and after the launch of the satellite till the solar panels are deployed To the spacecraft, its equipment, and payload during the shadow phase For communication and data transfer To maintain the electronics at a specific temperature Batteries with higher gravimetric(higher mass) and volumetric(higher volume) energy densities lead to lesser mass and volume for the power systems and thereby increase payload and mission capabilities.\nA Saft space battery pack comprised of VES180SA space cells. Credit: SpaceNews That is why Lithium-ion batteries are so extensively used in satellites as power systems. A NiCD(nickel-cadmium) battery pack has as much as 50% less energy density as Li-ion batteries.\nBatteries tend to degrade after continuous charge and discharge cycles. There is a need to develop an accurate remaining useful life(RUL) estimation system for Li-ion batteries. The estimated RUL can provide useful information to the \u0026lsquo;health management and maintenance\u0026rsquo; and the \u0026lsquo;ground reliability assessment\u0026rsquo; team. From this information, the maintenance team can plan the maintenance task schedule.\nThe approach of Lithium-ion battery RUL estimation Model-based approach Data-driven approach The model-based approach makes use of battery characteristics and physical structure. The data-driven approach is not based on accurately modeling the physics of a system like the model-based approach but instead estimates RUL based on historical data.\nI will be using the data-driven approach for this project.\nThe Dataset The dataset used for this project can be downloaded here. This data has been collected from prognostics testbed at the NASA Ames Prognostics Center of Excellence (PCoE). The experiments were conducted until a 30% drop in the battery capacity(from 2 A·h to 1.4 A·h) was observed. This time-series data can be used for the development of prognostic algorithms that can accurately predict the RUL of Li-ion batteries.\nData Structure: cycle:\ttop level structure array containing the charge, discharge and impedance operations type: operation type, can be charge, discharge or impedance ambient_temperature:\tambient temperature (degree C) time: the date and time of the start of the cycle, in MATLAB date vector format data:\tdata structure containing the measurements for charge the fields are: Voltage_measured: Battery terminal voltage (Volts) Current_measured:\tBattery output current (Amps) Temperature_measured: Battery temperature (degree C) Current_charge:\tCurrent measured at charger (Amps) Voltage_charge:\tVoltage measured at charger (Volts) Time:\tTime vector for the cycle (secs) for discharge the fields are: Voltage_measured: Battery terminal voltage (Volts) Current_measured:\tBattery output current (Amps) Temperature_measured: Battery temperature (degree C) Current_charge:\tCurrent measured at load (Amps) Voltage_charge:\tVoltage measured at load (Volts) Time:\tTime vector for the cycle (secs) Capacity:\tBattery capacity (Ahr) for discharge till 2.7V for impedance the fields are: Sense_current:\tCurrent in sense branch (Amps) Battery_current:\tCurrent in battery branch (Amps) Current_ratio:\tRatio of the above currents Battery_impedance:\tBattery impedance (Ohms) computed from raw data Rectified_impedance:\tCalibrated and smoothed battery impedance (Ohms) Re:\tEstimated electrolyte resistance (Ohms) Rct:\tEstimated charge transfer resistance (Ohms) Exploratory data analysis The battery data was provided in .mat format, I wrote a python script that parses .mat files and converts them into JSON objects. def create_correspondence_data(self): dom = PyQuery(self.url) writer = csv.writer(open(\u0026#39;correspondence.csv\u0026#39;, \u0026#39;wb\u0026#39;)) for i, img in enumerate(dom(\u0026#39;img\u0026#39;).items()): img_src = img.attr[\u0026#39;src\u0026#39;] print(\u0026#34;%d =\u0026gt; %s\u0026#34; % (i + 1, img_src)) writer.writerow([i + 1, img_src]) For visualization, I used the Matplotlib library.\nLet us take a look at the battery capacity degradation at different ambient temperatures.\nFrom the above diagrams, it can be noted that the capacity is not always continuously decreasing with every discharge cycle. Sometimes the capacity increases(eg cycle no 90 for the ambient temperature 24° C) resulting in distinctive spikes.\nThis is because of the self-charging effect of lithium-ion batteries. The explanation for this effect is during battery use chemical reactions take place, and the electrodes are deposited with chemical products, resulting in reduced chemical reactions. In order to melt the chemical products deposited, The battery requires a short rest period. The capacity of the battery increases suddenly owing to an increase in the available capacity in the next cycle.\nComparison of capacity degradation at different ambient temperature Plotting the capacity degradation data of the different ambient temperatures on a single plot we can see that the optimum temperature for the best performance of the battery is when the ambient temperature is 24°C since the capacity is higher than the battery maintained at 4°C and the degradation rate is lesser compared to the battery maintained at 43°C.\nCharging performance Let us now visualize the data and take a look at the charging profile of batteries at different temperatures.\nThe different columns of plots are:\nVoltage vs Time Current vs Time Temperature(Battery not Ambient temperature) vs Time Voltage vs Time The voltage increases non linearly until a point and then becomes constant. This is because the rated voltage of the battery is 4.2 so the voltage of the battery becomes constant at that point. This phase where the voltage increases non linearly is called the constant current phase here constant current is supplied to the battery till it becomes 4.2 volts. The phase where the voltage is constant is called the constant voltage phase since the voltage maintained here is constant.\nCurrent vs Time As mentioned previously, the constant current phase is a place where a constant current is supplied to the battery. During the constant voltage phase, the current declines non linearly in order to maintain a constant voltage. This is known as Trickle charging, where the fully charged battery is charged under load at a rate equal to its self-discharge rate, thus enabling the battery to remain at its fully charged level.\nTemperature vs Time Temperature is the highest at the point where the phase transitions from the constant current phase to the constant voltage phase.\nFindings It can be seen that the batteries maintained at a lower temperature seem to reach the maximum voltage a lot sooner. As the cycle number increases the battery charging time increases with the exception of the battery maintained at 43°C. Further analysis of this is needed. After looking at the graphs it can be seen that this is indeed true. The higher the clock number, the lesser is the charging time for the battery maintained at an ambient temperature of 43°C.\nDischarge performance Similarly, let us look at the discharge performance of batteries at different temperatures.\nThe different columns of plots are:\nVoltage vs Time Current vs Time Temperature(Battery not Ambient temperature) vs Time Voltage vs Time During a discharge cycle, the voltage discharges non linearly until the voltage becomes 0. After the voltage becomes 0, the voltage increases suddenly for a brief moment. This is due to the self-charging effect which was discussed earlier that increases the capacity of the battery for the next cycle. Current vs Time During a discharge cycle, a constant current is provided by the battery until the battery voltage becomes 0.\nTemperature vs Time Temperature is the highest when the battery voltage just reaches 0. After this phase, Self-charging phenomenon takes place due to the melting of deposits on the electrodes in the absence of chemical reactions. Findings It can be seen that the batteries maintained at an ambient temperature of 24°C last a lot longer during the discharge cycle compared to the other two ambient temperature. As the cycle number increases the battery charging time decreases with the exception of the battery maintained at 4°C. Further analysis of this is needed. After looking at the graphs it can be seen that this isn\u0026rsquo;t the case for all the cycles. The higher the clock number, the lesser is the discharging time, This makes sense as the capacity of the battery is known to decrease along with usage not increase. The capacity at cycle 0 was low because it was a new battery.\nModel building Now let us build a model to fit the data but first, we need to identify and remove outliers to increase prediction performance.\nThe outliers were detected using the rolling standard deviation method. Below is the python implementation of the rolling standard deviation method to identify outliers in the data.\ndef moving_average(data, window_size): \u0026#34;\u0026#34;\u0026#34; Computes moving average using discrete linear convolution of two one dimensional sequences. Args: ----- data (pandas.Series): independent variable window_size (int): rolling window size Returns: -------- ndarray of linear convolution API Reference: https://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html\u0026#34;\u0026#34;\u0026#34; window = np.ones(int(window_size))/float(window_size) return np.convolve(data, window, \u0026#39;same\u0026#39;) def rollingAverage(x_stuff, y_stuff): \u0026#34;\u0026#34;\u0026#34; Helps in exploring the anamolies using rolling standard deviation Args: ----- x_stuff (pandas.Series): X variable y_stuff (pandas.Series): Y variable Returns: -------- lst_x: List of X values where an outier was identified lst_y: List of y values where an outier was identified \u0026#34;\u0026#34;\u0026#34; window_size = 10 #rolling window size sigma=1.0 #value for standard deviation avg = moving_average(y_stuff, window_size) avg_list = avg.tolist() residual = y_stuff - avg # Calculate the variation in the distribution of the residual testing_std = residual.rolling(window_size).std() testing_std_as_df = pd.DataFrame(testing_std) rolling_std = testing_std_as_df.replace(np.nan, testing_std_as_df.iloc[window_size - 1]).round(3).iloc[:,0].tolist() rolling_std std = np.std(residual) lst=[] lst_index = 0 lst_count = 0 for i in y_stuff.index: if (y_stuff[i] \u0026gt; avg_list[lst_index] + (1.5 * rolling_std[lst_index])) | (y_stuff[i] \u0026lt; avg_list[lst_index] - (1.5 * rolling_std[lst_index])): lt=[i,x_stuff[i], y_stuff[i],avg_list[lst_index],rolling_std[lst_index]] lst.append(lt) lst_count+=1 lst_index+=1 lst_x = [] lst_y = [] for i in range (0,len(lst)): lst_x.append(lst[i][1]) lst_y.append(lst[i][2]) return lst_x, lst_y The plot of capacity degradation along with identified outliers.\nIdentifying whether to use a linear model vs a non-linear model for RUL estimation\nResidual plots can be useful to identify if a linear model is to be used or a non-linear.\nThe plot pattern that we obtain from the data is not random(U-shaped). This pattern indicates that a non-linear model is a better choice than a linear model to fit this data. For more information regarding residual plots visit this link.\nSupport vector regression Support vector machines can not only be used for the classification problem but also for regression. The kernel used is RBF(radial basis function).\nPython implementation of the code can be found below.\n# Import the library from sklearn.svm import SVR # Create a support vector regression model svr = SVR(C=20, epsilon=0.0001, gamma=0.00001, cache_size=200, kernel=\u0026#39;rbf\u0026#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False) svr.fit(X_train,y_train) # Fit the model y_pred = svr.predict(X_test) # Perform prediction Once the model is fit, The model can be used to estimate the capacity by taking cycle no as the input. The values of hyperparameters like C, epsilon, and gamma to obtain the best performance can be found out by using grid search and other parameter optimization techniques.\nNow let us compare the model by fitting it to different batteries with different train size.\nBattery 05 Battery 06 Battery 07 Battery 18 Conclusion Support vector regression can be used to build an accurate and effective RUL estimation system for Li-ion batteries, given that the hyperparameters of the SVR are properly chosen.\nAll the code and Jupyter notebook implementation of this project can be found on my GitHub here.\n","permalink":"https://potatospudowski.github.io/posts/rul/","summary":"Batteries are a critical part of any satellite\u0026rsquo;s power system.\nThey are used to provide power:\nDuring launch and after the launch of the satellite till the solar panels are deployed To the spacecraft, its equipment, and payload during the shadow phase For communication and data transfer To maintain the electronics at a specific temperature Batteries with higher gravimetric(higher mass) and volumetric(higher volume) energy densities lead to lesser mass and volume for the power systems and thereby increase payload and mission capabilities.","title":"RUL(Remaining Useful Life) and SOH(State of Health) estimation of Lithium-ion satellite power systems using Support-Vector-Regression"}]